{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56a0dfbd-af67-4e21-8062-ad6014039711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "MVP  STAR SCHEMA\n",
    "MVP Engenharia de dados Matrícula:4052025000027 \n",
    "Dataset: IEA Energy Demand and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09432b88-5330-4c6d-9a68-94114ea186e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A proposta deste trabalho é importar dados dos 10 estados que mais consomem energia elétrica nos estados unidos e analisar o perfil de geração elétrica de cada estado, olhando para as 3 principais fontes: Gás Natural, Carvão e Eólica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef7dd7c3-b681-451b-89f0-b3e993d57904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 1: IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde7f113-036d-4357-9c2c-ddcdbe2c0831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83D\uDCDA STEP 1: Importing libraries...\n✅ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "print(\"\\n\uD83D\uDCDA STEP 1: Importing libraries...\")\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "from pyspark.sql.functions import lit, current_timestamp, col, when\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "print(\"✅ Libraries imported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ea75ba-815d-40ba-986a-1bf7a7ad69af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 2: API CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a496b3-17cf-48d9-9da2-7bc41577bb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n\uD83D\uDCCA FETCHING ELECTRICITY DATA FOR 10 US STATES\n================================================================================\n\n\uD83D\uDCC5 Configuring date range...\n   Start Date: 2025-11-22\n   End Date: 2025-12-22\n   Total Days: 30\n\n\uD83D\uDD27 Setting up EIA API...\n\n\uD83C\uDFAF Target States and Their Balancing Authority Codes:\n------------------------------------------------------------\n  • California           → CAL, CISO\n  • Texas                → ERCO\n  • Florida              → FPL, TECO, FPC\n  • Ohio                 → FE, PJM\n  • Georgia              → SOCO\n  • New York             → NY, NYIS\n  • Pennsylvania         → PJM, FE\n  • North Carolina       → DUK, CAR\n  • Virginia             → PJM\n  • Illinois             → MISO\n\n\uD83D\uDCCA Total unique BA codes to try: 14\n   Codes: CAL, CAR, CISO, DUK, ERCO, FE, FPC, FPL, MISO, NY, NYIS, PJM, SOCO, TECO\n\n\uD83D\uDD0D Checking available respondent codes from EIA API...\n   Getting respondent facet values...\n   ✅ Found 81 respondent codes total\n\n   Checking availability of our target BA codes:\n      ✅ CISO: California Independent System Operator\n      ✅ FPC: Duke Energy Florida, Inc.\n      ✅ PJM: PJM Interconnection, LLC\n      ✅ NYIS: New York Independent System Operator\n      ✅ SOCO: Southern Company Services, Inc. - Trans\n      ✅ DUK: Duke Energy Carolinas\n      ✅ FPL: Florida Power & Light Co.\n      ✅ CAR: Carolinas\n      ✅ ERCO: Electric Reliability Council of Texas, Inc.\n      ✅ CAL: California\n      ✅ NY: New York\n      ❌ TECO: Not found in API\n      ❌ FE: Not found in API\n      ✅ MISO: Midcontinent Independent System Operator, Inc.\n\n   \uD83D\uDCCB Availability Summary:\n      • Available: 12/14\n      • Unavailable: 2\n\n   ✅ Using respondent codes: ['CISO', 'FPC', 'PJM', 'NYIS', 'SOCO', 'DUK', 'FPL', 'CAR', 'ERCO', 'CAL', 'NY', 'MISO']\n\n================================================================================\n\uD83D\uDCE5 FETCHING ELECTRICITY DATA\n================================================================================\n\n\uD83D\uDCCB Fetching datasets for all available BA codes:\n------------------------------------------------------------\n\n============================================================\n\uD83D\uDCCA Electricity Demand\n============================================================\n\n\uD83D\uDCCD BA Code: CISO\n   State: California\n   ✅ SUCCESS: 721 records\n   \uD83D\uDCC8 Sample values: 23387 to 26510 MWh\n\n\uD83D\uDCCD BA Code: FPC\n   State: Florida\n   ✅ SUCCESS: 721 records\n   \uD83D\uDCC8 Sample values: 5393 to 6923 MWh\n\n\uD83D\uDCCD BA Code: PJM\n   State: Ohio\n   ✅ SUCCESS: 721 records\n   \uD83D\uDCC8 Sample values: 87341 to 94806 MWh\n\n\uD83D\uDCCD BA Code: NYIS\n   State: New York\n   ✅ SUCCESS: 721 records\n   \uD83D\uDCC8 Sample values: 16093 to 18484 MWh\n\n\uD83D\uDCCD BA Code: SOCO\n   State: Georgia\n   ✅ SUCCESS: 721 records\n   \uD83D\uDCC8 Sample values: 23290 to 25883 MWh\n\n\uD83D\uDCCD BA Code: DUK\n   State: North Carolina\n   ✅ SUCCESS: 721 records\n   \uD83D\uDCC8 Sample values: 10209 to 11656 MWh\n\n\uD83D\uDCCD BA Code: FPL\n   State: Florida\n   ✅ SUCCESS: 721 records\n   \uD83D\uDCC8 Sample values: 14064 to 17450 MWh\n\n\uD83D\uDCCD BA Code: CAR\n   State: North Carolina\n   ✅ SUCCESS: 721 records\n   \uD83D\uDCC8 Sample values: 18984 to 21686 MWh\n\n\uD83D\uDCCD BA Code: ERCO\n   State: Texas\n   ✅ SUCCESS: 721 records\n   \uD83D\uDCC8 Sample values: 51470 to 57382 MWh\n\n\uD83D\uDCCD BA Code: CAL\n   State: California\n   ✅ SUCCESS: 721 records\n   \uD83D\uDCC8 Sample values: 28496 to 31905 MWh\n\n\uD83D\uDCCD BA Code: NY\n   State: New York\n   ✅ SUCCESS: 721 records\n   \uD83D\uDCC8 Sample values: 16093 to 18484 MWh\n\n\uD83D\uDCCD BA Code: MISO\n   State: Illinois\n   ✅ SUCCESS: 721 records\n   \uD83D\uDCC8 Sample values: 70194 to 75908 MWh\n\n============================================================\n\uD83D\uDCCA Coal Generation\n============================================================\n\n\uD83D\uDCCD BA Code: CISO\n   State: California\n   ✅ SUCCESS: 704 records\n   \uD83D\uDCC8 Sample values: 0 to 0 MWh\n\n\uD83D\uDCCD BA Code: FPC\n   State: Florida\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 422 to 595 MWh\n\n\uD83D\uDCCD BA Code: PJM\n   State: Ohio\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 15100 to 16326 MWh\n\n\uD83D\uDCCD BA Code: NYIS\n   State: New York\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 0 to 0 MWh\n\n\uD83D\uDCCD BA Code: SOCO\n   State: Georgia\n   ✅ SUCCESS: 702 records\n   \uD83D\uDCC8 Sample values: 5471 to 5606 MWh\n\n\uD83D\uDCCD BA Code: DUK\n   State: North Carolina\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 734 to 739 MWh\n\n\uD83D\uDCCD BA Code: FPL\n   State: Florida\n   ⚠️ No records returned\n\n\uD83D\uDCCD BA Code: CAR\n   State: North Carolina\n   ✅ SUCCESS: 704 records\n   \uD83D\uDCC8 Sample values: 1716 to 1868 MWh\n\n\uD83D\uDCCD BA Code: ERCO\n   State: Texas\n   ✅ SUCCESS: 678 records\n   \uD83D\uDCC8 Sample values: 7689 to 7877 MWh\n\n\uD83D\uDCCD BA Code: CAL\n   State: California\n   ✅ SUCCESS: 704 records\n   \uD83D\uDCC8 Sample values: 545 to 588 MWh\n\n\uD83D\uDCCD BA Code: NY\n   State: New York\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 0 to 0 MWh\n\n\uD83D\uDCCD BA Code: MISO\n   State: Illinois\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 22972 to 24272 MWh\n\n============================================================\n\uD83D\uDCCA Natural Gas Generation\n============================================================\n\n\uD83D\uDCCD BA Code: CISO\n   State: California\n   ✅ SUCCESS: 704 records\n   \uD83D\uDCC8 Sample values: 11297 to 12127 MWh\n\n\uD83D\uDCCD BA Code: FPC\n   State: Florida\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 3971 to 5189 MWh\n\n\uD83D\uDCCD BA Code: PJM\n   State: Ohio\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 35940 to 40212 MWh\n\n\uD83D\uDCCD BA Code: NYIS\n   State: New York\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 7412 to 8504 MWh\n\n\uD83D\uDCCD BA Code: SOCO\n   State: Georgia\n   ✅ SUCCESS: 702 records\n   \uD83D\uDCC8 Sample values: 11577 to 12126 MWh\n\n\uD83D\uDCCD BA Code: DUK\n   State: North Carolina\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 2234 to 3389 MWh\n\n\uD83D\uDCCD BA Code: FPL\n   State: Florida\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 9999 to 13382 MWh\n\n\uD83D\uDCCD BA Code: CAR\n   State: North Carolina\n   ✅ SUCCESS: 704 records\n   \uD83D\uDCC8 Sample values: 5206 to 6586 MWh\n\n\uD83D\uDCCD BA Code: ERCO\n   State: Texas\n   ✅ SUCCESS: 678 records\n   \uD83D\uDCC8 Sample values: 31979 to 32433 MWh\n\n\uD83D\uDCCD BA Code: CAL\n   State: California\n   ✅ SUCCESS: 704 records\n   \uD83D\uDCC8 Sample values: 14809 to 15507 MWh\n\n\uD83D\uDCCD BA Code: NY\n   State: New York\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 7412 to 8504 MWh\n\n\uD83D\uDCCD BA Code: MISO\n   State: Illinois\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 29484 to 35378 MWh\n\n============================================================\n\uD83D\uDCCA Wind Generation\n============================================================\n\n\uD83D\uDCCD BA Code: CISO\n   State: California\n   ✅ SUCCESS: 704 records\n   \uD83D\uDCC8 Sample values: 1168 to 1574 MWh\n\n\uD83D\uDCCD BA Code: FPC\n   State: Florida\n   ⚠️ No records returned\n\n\uD83D\uDCCD BA Code: PJM\n   State: Ohio\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 1745 to 2164 MWh\n\n\uD83D\uDCCD BA Code: NYIS\n   State: New York\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 298 to 814 MWh\n\n\uD83D\uDCCD BA Code: SOCO\n   State: Georgia\n   ✅ SUCCESS: 702 records\n   \uD83D\uDCC8 Sample values: 0 to 0 MWh\n\n\uD83D\uDCCD BA Code: DUK\n   State: North Carolina\n   ⚠️ No records returned\n\n\uD83D\uDCCD BA Code: FPL\n   State: Florida\n   ⚠️ No records returned\n\n\uD83D\uDCCD BA Code: CAR\n   State: North Carolina\n   ⚠️ No records returned\n\n\uD83D\uDCCD BA Code: ERCO\n   State: Texas\n   ✅ SUCCESS: 678 records\n   \uD83D\uDCC8 Sample values: 6501 to 7282 MWh\n\n\uD83D\uDCCD BA Code: CAL\n   State: California\n   ✅ SUCCESS: 704 records\n   \uD83D\uDCC8 Sample values: 1253 to 1716 MWh\n\n\uD83D\uDCCD BA Code: NY\n   State: New York\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 298 to 814 MWh\n\n\uD83D\uDCCD BA Code: MISO\n   State: Illinois\n   ✅ SUCCESS: 701 records\n   \uD83D\uDCC8 Sample values: 5579 to 9926 MWh\n\n================================================================================\n\uD83D\uDD04 CONVERTING TO DATAFRAMES\n================================================================================\n\n\uD83D\uDCE6 Converting 86 datasets to DataFrames...\n\n✅ Conversion complete:\n   • Successful: 86\n   • Failed: 0\n   • Total DataFrames: 129\n\n================================================================================\n\uD83D\uDCCA FETCHING RESULTS SUMMARY\n================================================================================\n\n\uD83D\uDDFA️ DATA BY STATE:\n------------------------------------------------------------\n         State  Data Types  Total Records  BA Codes\n    California           4           5666 CAL, CISO\n       Florida           3           3545  FPC, FPL\n       Georgia           4           2827      SOCO\n      Illinois           4           2824      MISO\n      New York           4           5648  NY, NYIS\nNorth Carolina           3           4252  CAR, DUK\n          Ohio           4           2824       PJM\n         Texas           4           2755      ERCO\n\n⚡ DATA BY TYPE:\n------------------------------------------------------------\n             Data Type  States  Total Records  Datasets\n       coal_generation       8           7698        11\n    electricity_demand       8           8652        12\nnatural_gas_generation       8           8399        12\n       wind_generation       6           5592         8\n\n\uD83D\uDCC8 OVERALL TOTALS:\n   • Total fetch attempts: 48\n   • Successful fetches: 43\n   • Total records: 30,341\n   • States with data: 8\n\n================================================================================\n\uD83D\uDC41️ SAMPLE DATA PREVIEW\n================================================================================\n\n\uD83C\uDFDB️ CALIFORNIA:\n   Available datasets: 16\n\n   \uD83D\uDCCA electricity_California_CISO:\n      Shape: (721, 7)\n      Time range: 2025-11-22T00 to 2025-12-22T00\n      Value range: 19183 to 30792 MWh\n      Average: 24901 MWh\n\n      First 2 rows:\n          period respondent                         respondent-name type type-name  value    value-units\n0  2025-11-22T00       CISO  California Independent System Operator    D    Demand  23387  megawatthours\n1  2025-11-22T01       CISO  California Independent System Operator    D    Demand  23486  megawatthours\n\n\uD83C\uDFDB️ TEXAS:\n   Available datasets: 8\n\n   \uD83D\uDCCA electricity_Texas_ERCO:\n      Shape: (721, 7)\n      Time range: 2025-11-22T00 to 2025-12-22T00\n      Value range: 41150 to 63114 MWh\n      Average: 49842 MWh\n\n      First 2 rows:\n          period respondent                              respondent-name type type-name  value    value-units\n0  2025-11-22T00       ERCO  Electric Reliability Council of Texas, Inc.    D    Demand  57382  megawatthours\n1  2025-11-22T01       ERCO  Electric Reliability Council of Texas, Inc.    D    Demand  56384  megawatthours\n\n\uD83C\uDFDB️ FLORIDA:\n   Available datasets: 10\n\n   \uD83D\uDCCA electricity_Florida_FPC:\n      Shape: (721, 7)\n      Time range: 2025-11-22T00 to 2025-12-22T00\n      Value range: 3756 to 7786 MWh\n      Average: 5438 MWh\n\n      First 2 rows:\n          period respondent            respondent-name type type-name value    value-units\n0  2025-11-22T00        FPC  Duke Energy Florida, Inc.    D    Demand  6923  megawatthours\n1  2025-11-22T01        FPC  Duke Energy Florida, Inc.    D    Demand  6465  megawatthours\n\n\uD83C\uDFDB️ NEW YORK:\n   Available datasets: 16\n\n   \uD83D\uDCCA electricity_New_York_NYIS:\n      Shape: (721, 7)\n      Time range: 2025-11-22T00 to 2025-12-22T00\n      Value range: 13385 to 23448 MWh\n      Average: 17999 MWh\n\n      First 2 rows:\n          period respondent                       respondent-name type type-name  value    value-units\n0  2025-11-22T00       NYIS  New York Independent System Operator    D    Demand  18484  megawatthours\n1  2025-11-22T01       NYIS  New York Independent System Operator    D    Demand  18055  megawatthours\n\n\uD83C\uDFDB️ ILLINOIS:\n   Available datasets: 8\n\n   \uD83D\uDCCA electricity_Illinois_MISO:\n      Shape: (721, 7)\n      Time range: 2025-11-22T00 to 2025-12-22T00\n      Value range: 58796 to 99092 MWh\n      Average: 77143 MWh\n\n      First 2 rows:\n          period respondent                                 respondent-name type type-name  value    value-units\n0  2025-11-22T00       MISO  Midcontinent Independent System Operator, Inc.    D    Demand  75908  megawatthours\n1  2025-11-22T01       MISO  Midcontinent Independent System Operator, Inc.    D    Demand  75012  megawatthours\n\n================================================================================\n\uD83D\uDCCB ALL AVAILABLE DATAFRAMES\n================================================================================\n\nTotal DataFrames: 129\n\nFirst 20 DataFrames:\n  1. coal_California_CAL                                |    704 records\n  2. coal_California_CISO                               |    704 records\n  3. coal_Florida_FPC                                   |    701 records\n  4. coal_Georgia_SOCO                                  |    702 records\n  5. coal_Illinois_MISO                                 |    701 records\n  6. coal_New_York_NY                                   |    701 records\n  7. coal_New_York_NYIS                                 |    701 records\n  8. coal_North_Carolina_CAR                            |    704 records\n  9. coal_North_Carolina_DUK                            |    701 records\n 10. coal_Ohio_PJM                                      |    701 records\n 11. coal_Texas_ERCO                                    |    678 records\n 12. coal_generation_CAL                                |    704 records\n 13. coal_generation_CAR                                |    704 records\n 14. coal_generation_CISO                               |    704 records\n 15. coal_generation_California_CAL                     |    704 records\n 16. coal_generation_California_CISO                    |    704 records\n 17. coal_generation_DUK                                |    701 records\n 18. coal_generation_ERCO                               |    678 records\n 19. coal_generation_FPC                                |    701 records\n 20. coal_generation_Florida_FPC                        |    701 records\n\n================================================================================\n\uD83D\uDE80 HOW TO ACCESS YOUR DATA\n================================================================================\n\nACCESS EXAMPLES:\n----------------\n\n1. By State and Data Type (Recommended):\n   -------------------------------------\n   # Texas electricity demand\n   df = dataframes['electricity_demand_Texas_ERCO']\n   \n   # California natural gas generation  \n   df = dataframes['natural_gas_generation_California_CAL']\n   \n   # Florida wind generation\n   df = dataframes['wind_generation_Florida_FPL']\n   \n   # Illinois coal generation\n   df = dataframes['coal_generation_Illinois_MISO']\n\n2. By BA Code Only:\n   -----------------\n   df = dataframes['electricity_demand_ERCO']\n   df = dataframes['coal_generation_PJM']\n   df = dataframes['wind_generation_MISO']\n\n3. Check All Available:\n   ---------------------\n   print(\"Available DataFrames:\", len(dataframes))\n   print(\"Keys starting with 'Texas':\")\n   texas_keys = [k for k in dataframes.keys() if 'Texas' in k]\n   for key in texas_keys:\n       print(f\"  • {key}\")\n\nQUICK ANALYSIS EXAMPLES:\n------------------------\n# 1. Get Texas demand statistics\nif 'electricity_demand_Texas_ERCO' in dataframes:\n    df = dataframes['electricity_demand_Texas_ERCO']\n    df['value_numeric'] = pd.to_numeric(df['value'], errors='coerce')\n    print(\"Texas Demand Stats:\")\n    print(f\"  Min: {df['value_numeric'].min():.0f} MWh\")\n    print(f\"  Max: {df['value_numeric'].max():.0f} MWh\") \n    print(f\"  Mean: {df['value_numeric'].mean():.0f} MWh\")\n\n# 2. Compare states\nstates = ['California', 'Texas', 'Florida']\nfor state in states:\n    key = f'electricity_demand_{state.replace(\" \", \"_\")}'\n    matching_keys = [k for k in dataframes.keys() if key in k]\n    if matching_keys:\n        df = dataframes[matching_keys[0]]\n        avg_demand = pd.to_numeric(df['value'], errors='coerce').mean()\n        print(f\"{state}: {avg_demand:.0f} MWh average demand\")\n\n\n================================================================================\n✅ IMPORT COMPLETE - NEXT STEPS\n================================================================================\n\n\uD83C\uDF89 SUCCESS! Your electricity data has been imported.\n\n\uD83D\uDCCA WHAT YOU HAVE:\n   • 129 DataFrames ready for analysis\n   • Data for 8 states\n   • Hourly data from 2025-11-22 to 2025-12-22\n   • Four data types: Demand, Coal, Natural Gas, Wind\n\n\uD83D\uDD27 TO GET MORE DATA:\n   1. Increase date range (up to 5000 records limit):\n      start_date = end_date - timedelta(days=208)  # Max ~208 days\n   \n   2. Try additional BA codes:\n      # Add these to state_ba_mapping\n      \"New York\": [\"NY\", \"NYIS\", \"PJM\"],\n      \"Pennsylvania\": [\"PJM\", \"FE\", \"NY\"],\n      \"Virginia\": [\"PJM\", \"NY\"]\n\n\uD83D\uDCC8 READY FOR ANALYSIS:\n   Start with: dataframes['electricity_demand_California_CAL'].head()\n\n\n================================================================================\n\uD83E\uDDEA QUICK TEST - RUN THESE COMMANDS\n================================================================================\n\n# Test 1: Check what data we have\nprint(f\"Total DataFrames: {len(dataframes)}\")\n\n# Test 2: List all Texas data\ntexas_data = [k for k in dataframes.keys() if 'Texas' in k]\nprint(f\"Texas datasets: {len(texas_data)}\")\nfor key in texas_data[:5]:\n    print(f\"  • {key}\")\n\n# Test 3: View a sample DataFrame\nif dataframes:\n    sample_key = list(dataframes.keys())[0]\n    print(f\"\nSample: {sample_key}\")\n    print(dataframes[sample_key].head(2))\n\n\n================================================================================\n✅ ALL DONE! YOUR DATA IS READY FOR ANALYSIS\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Use the full workspace path for your notebook\n",
    "dbutils.notebook.run(\n",
    "    \"/Workspace/Users/lucasrobertideoliveira@hotmail.com/SETUP SECRETS\",\n",
    "    60\n",
    ")\n",
    "#  ============================================================================\n",
    "# \uD83D\uDCCA FETCH ELECTRICITY DATA FOR 10 US STATES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83D\uDCCA FETCHING ELECTRICITY DATA FOR 10 US STATES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CONFIGURE DATE RANGE\n",
    "# ============================================================================\n",
    "print(\"\\n\uD83D\uDCC5 Configuring date range...\")\n",
    "end_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "start_date = end_date - timedelta(days=30)  # 30 days for testing\n",
    "\n",
    "print(f\"   Start Date: {start_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"   End Date: {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"   Total Days: {(end_date - start_date).days}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. SETUP EIA API CONFIGURATION (DATABRICKS SECRETS VERSION)\n",
    "# ============================================================================\n",
    "print(\"\\n\uD83D\uDD27 Setting up EIA API...\")\n",
    "\n",
    "def load_eia_api_key():\n",
    "    try:\n",
    "        api_key = dbutils.secrets.get(\n",
    "            scope=\"eia-scope\",\n",
    "            key=\"eia_api_key\"\n",
    "        )\n",
    "        if api_key and api_key.strip():\n",
    "            masked = api_key[:5] + \"...\" + api_key[-5:] if len(api_key) > 10 else \"***\"\n",
    "            print(f\"   ✅ Loaded from Databricks Secrets: {masked}\")\n",
    "            return api_key.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    import os\n",
    "    api_key = os.environ.get(\"EIA_API_KEY\")\n",
    "    if api_key and api_key.strip():\n",
    "        masked = api_key[:5] + \"...\" + api_key[-5:] if len(api_key) > 10 else \"***\"\n",
    "        print(f\"   ✅ Loaded from environment variable: {masked}\")\n",
    "        return api_key.strip()\n",
    "\n",
    "    # Check if variable is defined from SETUP_SECRETS\n",
    "    try:\n",
    "        api_key = YOUR_REAL_EIA_API_KEY\n",
    "        if api_key and api_key.strip():\n",
    "            masked = api_key[:5] + \"...\" + api_key[-5:] if len(api_key) > 10 else \"***\"\n",
    "            print(f\"   ✅ Loaded from variable: {masked}\")\n",
    "            return api_key.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"\\n   ❌ ERROR: No API key found in any source!\")\n",
    "    raise ValueError(\"EIA API Key is required. Run SETUP_SECRETS notebook first.\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TARGET STATES AND THEIR BA CODES\n",
    "# ============================================================================\n",
    "print(\"\\n\uD83C\uDFAF Target States and Their Balancing Authority Codes:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Based on your working codes and EIA documentation\n",
    "state_ba_mapping = {\n",
    "    # California - Using CAL (California) - this works\n",
    "    \"California\": [\"CAL\", \"CISO\"],\n",
    "    \n",
    "    # Texas - Using ERCO (Electric Reliability Council of Texas) - this works\n",
    "    \"Texas\": [\"ERCO\"],\n",
    "    \n",
    "    # Florida - Using FPL (Florida Power & Light) - this works\n",
    "    \"Florida\": [\"FPL\", \"TECO\", \"FPC\"],\n",
    "    \n",
    "    # Ohio - Part of PJM, using FE (FirstEnergy) - this works\n",
    "    \"Ohio\": [\"FE\", \"PJM\"],\n",
    "    \n",
    "    # Georgia - Using SOCO (Southern Company) - this works\n",
    "    \"Georgia\": [\"SOCO\"],\n",
    "    \n",
    "    # New York - Using NY (New York) and NYIS (New York ISO)\n",
    "    \"New York\": [\"NY\", \"NYIS\"],\n",
    "    \n",
    "    # Pennsylvania - Part of PJM, using PJM\n",
    "    \"Pennsylvania\": [\"PJM\", \"FE\"],\n",
    "    \n",
    "    # North Carolina - Using DUK (Duke Energy) and CAR (Carolinas)\n",
    "    \"North Carolina\": [\"DUK\", \"CAR\"],\n",
    "    \n",
    "    # Virginia - Part of PJM, using PJM\n",
    "    \"Virginia\": [\"PJM\"],\n",
    "    \n",
    "    # Illinois - Using MISO (Midcontinent ISO)\n",
    "    \"Illinois\": [\"MISO\"]\n",
    "}\n",
    "\n",
    "# Display mapping\n",
    "for state, bas in state_ba_mapping.items():\n",
    "    print(f\"  • {state:20} → {', '.join(bas)}\")\n",
    "\n",
    "# Get all unique BA codes\n",
    "all_ba_codes = []\n",
    "for bas in state_ba_mapping.values():\n",
    "    all_ba_codes.extend(bas)\n",
    "all_ba_codes = list(set(all_ba_codes))\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA Total unique BA codes to try: {len(all_ba_codes)}\")\n",
    "print(f\"   Codes: {', '.join(sorted(all_ba_codes))}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CHECK AVAILABLE RESPONDENT CODES\n",
    "# ============================================================================\n",
    "print(\"\\n\uD83D\uDD0D Checking available respondent codes from EIA API...\")\n",
    "respondent_facet_url = f\"{EIA_BASE_URL}/electricity/rto/region-data/facet/respondent\"\n",
    "respondent_params = {\"api_key\": EIA_API_KEY}\n",
    "\n",
    "available_respondents = {}\n",
    "try:\n",
    "    print(f\"   Getting respondent facet values...\")\n",
    "    respondent_response = requests.get(respondent_facet_url, params=respondent_params, timeout=10)\n",
    "    \n",
    "    if respondent_response.status_code == 200:\n",
    "        respondent_data = respondent_response.json()\n",
    "        \n",
    "        if 'response' in respondent_data and 'facets' in respondent_data['response']:\n",
    "            facets = respondent_data['response']['facets']\n",
    "            print(f\"   ✅ Found {len(facets)} respondent codes total\")\n",
    "            \n",
    "            # Map all available respondents\n",
    "            for facet in facets:\n",
    "                available_respondents[facet['id']] = facet.get('name', 'No name')\n",
    "            \n",
    "            # Check which of our target codes are available\n",
    "            print(\"\\n   Checking availability of our target BA codes:\")\n",
    "            available_codes = []\n",
    "            unavailable_codes = []\n",
    "            \n",
    "            for ba_code in all_ba_codes:\n",
    "                if ba_code in available_respondents:\n",
    "                    available_codes.append(ba_code)\n",
    "                    print(f\"      ✅ {ba_code}: {available_respondents[ba_code]}\")\n",
    "                else:\n",
    "                    unavailable_codes.append(ba_code)\n",
    "                    print(f\"      ❌ {ba_code}: Not found in API\")\n",
    "            \n",
    "            print(f\"\\n   \uD83D\uDCCB Availability Summary:\")\n",
    "            print(f\"      • Available: {len(available_codes)}/{len(all_ba_codes)}\")\n",
    "            print(f\"      • Unavailable: {len(unavailable_codes)}\")\n",
    "            \n",
    "            # Use only available codes\n",
    "            selected_respondents = available_codes\n",
    "            \n",
    "            # If we don't have enough, add some fallbacks from known working codes\n",
    "            if len(selected_respondents) < 5:\n",
    "                print(\"\\n   ⚠️ Few codes available. Adding known working codes...\")\n",
    "                fallback_codes = [\"CAL\", \"ERCO\", \"SOCO\", \"FE\", \"FPL\", \"CISO\", \"NY\", \"PJM\", \"DUK\", \"MISO\"]\n",
    "                for code in fallback_codes:\n",
    "                    if code not in selected_respondents and code in available_respondents:\n",
    "                        selected_respondents.append(code)\n",
    "                        print(f\"      ➕ Added: {code}\")\n",
    "            \n",
    "            print(f\"\\n   ✅ Using respondent codes: {selected_respondents}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"   ⚠️ Could not parse respondent data\")\n",
    "            # Use known working codes as fallback\n",
    "            selected_respondents = [\"CAL\", \"ERCO\", \"SOCO\", \"FE\", \"FPL\", \"CISO\", \"NY\", \"PJM\", \"DUK\", \"MISO\"]\n",
    "            print(f\"   ⚠️ Using fallback codes: {selected_respondents}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"   ❌ Respondent facet failed: {respondent_response.status_code}\")\n",
    "        # Use known working codes as fallback\n",
    "        selected_respondents = [\"CAL\", \"ERCO\", \"SOCO\", \"FE\", \"FPL\", \"CISO\", \"NY\", \"PJM\", \"DUK\", \"MISO\"]\n",
    "        print(f\"   ⚠️ Using fallback codes: {selected_respondents}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Error getting respondent codes: {e}\")\n",
    "    # Use known working codes as fallback\n",
    "    selected_respondents = [\"CAL\", \"ERCO\", \"SOCO\", \"FE\", \"FPL\", \"CISO\", \"NY\", \"PJM\", \"DUK\", \"MISO\"]\n",
    "    print(f\"   ⚠️ Using fallback codes: {selected_respondents}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. FETCH DATA FUNCTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83D\uDCE5 FETCHING ELECTRICITY DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "base_params = {\n",
    "    \"api_key\": EIA_API_KEY,\n",
    "    \"frequency\": \"hourly\",\n",
    "    \"data[]\": \"value\",\n",
    "    \"start\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "    \"end\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "    \"sort[0][column]\": \"period\",\n",
    "    \"sort[0][direction]\": \"asc\",\n",
    "    \"offset\": 0,\n",
    "    \"length\": 5000\n",
    "}\n",
    "\n",
    "datasets_config = {\n",
    "    \"electricity_demand\": {\n",
    "        \"endpoint\": \"electricity/rto/region-data/data\",\n",
    "        \"params\": base_params.copy(),\n",
    "        \"facets[type][]\": \"D\"\n",
    "    },\n",
    "    \"coal_generation\": {\n",
    "        \"endpoint\": \"electricity/rto/fuel-type-data/data\",\n",
    "        \"params\": base_params.copy(),\n",
    "        \"facets[fueltype][]\": \"COL\"\n",
    "    },\n",
    "    \"natural_gas_generation\": {\n",
    "        \"endpoint\": \"electricity/rto/fuel-type-data/data\",\n",
    "        \"params\": base_params.copy(),\n",
    "        \"facets[fueltype][]\": \"NG\"\n",
    "    },\n",
    "    \"wind_generation\": {\n",
    "        \"endpoint\": \"electricity/rto/fuel-type-data/data\",\n",
    "        \"params\": base_params.copy(),\n",
    "        \"facets[fueltype][]\": \"WND\"\n",
    "    }\n",
    "}\n",
    "\n",
    "fetched_data = {}\n",
    "summary = []\n",
    "state_data = {}  # To organize data by state\n",
    "\n",
    "print(\"\\n\uD83D\uDCCB Fetching datasets for all available BA codes:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, config in datasets_config.items():\n",
    "    data_type = name.replace('_', ' ').title()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"\uD83D\uDCCA {data_type}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    for respondent in selected_respondents:\n",
    "        print(f\"\\n\uD83D\uDCCD BA Code: {respondent}\")\n",
    "        \n",
    "        # Find which state this BA belongs to\n",
    "        state_name = None\n",
    "        for state, bas in state_ba_mapping.items():\n",
    "            if respondent in bas:\n",
    "                state_name = state\n",
    "                break\n",
    "        \n",
    "        if state_name:\n",
    "            print(f\"   State: {state_name}\")\n",
    "        \n",
    "        params = config[\"params\"].copy()\n",
    "        params[\"facets[respondent][]\"] = respondent\n",
    "        for key, value in config.items():\n",
    "            if key.startswith(\"facets[\") and key != \"facets[respondent][]\":\n",
    "                params[key] = value\n",
    "        \n",
    "        endpoint = config[\"endpoint\"]\n",
    "        url = f\"{EIA_BASE_URL}/{endpoint}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                records = data.get('response', {}).get('data', [])\n",
    "                \n",
    "                if records and len(records) > 0:\n",
    "                    print(f\"   ✅ SUCCESS: {len(records)} records\")\n",
    "                    \n",
    "                    # Create keys for storage\n",
    "                    raw_key = f\"{name}_{respondent}\"\n",
    "                    state_key = f\"{name}_{state_name.replace(' ', '_')}_{respondent}\" if state_name else raw_key\n",
    "                    \n",
    "                    # Store the data\n",
    "                    fetched_data[raw_key] = records\n",
    "                    fetched_data[state_key] = records  # Also store with state name\n",
    "                    \n",
    "                    # Organize by state\n",
    "                    if state_name:\n",
    "                        if state_name not in state_data:\n",
    "                            state_data[state_name] = {}\n",
    "                        if name not in state_data[state_name]:\n",
    "                            state_data[state_name][name] = []\n",
    "                        state_data[state_name][name].append({\n",
    "                            'ba_code': respondent,\n",
    "                            'records': records,\n",
    "                            'count': len(records)\n",
    "                        })\n",
    "                    \n",
    "                    # Show sample stats\n",
    "                    df = pd.DataFrame(records[:5])  # Just first 5 for stats\n",
    "                    if 'value' in df.columns:\n",
    "                        values = pd.to_numeric(df['value'], errors='coerce')\n",
    "                        valid_values = values.dropna()\n",
    "                        if len(valid_values) > 0:\n",
    "                            print(f\"   \uD83D\uDCC8 Sample values: {valid_values.min():.0f} to {valid_values.max():.0f} MWh\")\n",
    "                    \n",
    "                    summary.append({\n",
    "                        'dataset': state_key if state_name else raw_key,\n",
    "                        'state': state_name if state_name else 'Unknown',\n",
    "                        'ba_code': respondent,\n",
    "                        'data_type': name,\n",
    "                        'records': len(records),\n",
    "                        'status': 'SUCCESS'\n",
    "                    })\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"   ⚠️ No records returned\")\n",
    "                    summary.append({\n",
    "                        'dataset': f\"{name}_{respondent}\",\n",
    "                        'state': state_name if state_name else 'Unknown',\n",
    "                        'ba_code': respondent,\n",
    "                        'data_type': name,\n",
    "                        'records': 0,\n",
    "                        'status': 'NO_DATA'\n",
    "                    })\n",
    "                    \n",
    "            else:\n",
    "                print(f\"   ❌ HTTP Error {response.status_code}\")\n",
    "                summary.append({\n",
    "                    'dataset': f\"{name}_{respondent}\",\n",
    "                    'state': state_name if state_name else 'Unknown',\n",
    "                    'ba_code': respondent,\n",
    "                    'data_type': name,\n",
    "                    'records': 0,\n",
    "                    'status': f'HTTP_{response.status_code}'\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error: {str(e)[:100]}\")\n",
    "            summary.append({\n",
    "                'dataset': f\"{name}_{respondent}\",\n",
    "                'state': state_name if state_name else 'Unknown',\n",
    "                'ba_code': respondent,\n",
    "                'data_type': name,\n",
    "                'records': 0,\n",
    "                'status': 'ERROR'\n",
    "            })\n",
    "\n",
    "# ============================================================================\n",
    "# 6. CONVERT TO DATAFRAMES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83D\uDD04 CONVERTING TO DATAFRAMES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "if fetched_data:\n",
    "    print(f\"\\n\uD83D\uDCE6 Converting {len(fetched_data)} datasets to DataFrames...\")\n",
    "    \n",
    "    conversion_stats = {'success': 0, 'failed': 0}\n",
    "    \n",
    "    for key, records in fetched_data.items():\n",
    "        try:\n",
    "            if records and len(records) > 0:\n",
    "                df = pd.DataFrame(records)\n",
    "                dataframes[key] = df\n",
    "                conversion_stats['success'] += 1\n",
    "                \n",
    "                # Create simplified aliases for easier access\n",
    "                parts = key.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    # Format: type_state_ba or type_ba\n",
    "                    if any(state.replace(' ', '_') in key for state in state_ba_mapping.keys()):\n",
    "                        # Already has state name\n",
    "                        pass\n",
    "                    else:\n",
    "                        # Try to add state name\n",
    "                        ba_code = parts[-1]\n",
    "                        for state, bas in state_ba_mapping.items():\n",
    "                            if ba_code in bas:\n",
    "                                state_alias = f\"{parts[0]}_{state.replace(' ', '_')}_{ba_code}\"\n",
    "                                if state_alias not in dataframes:\n",
    "                                    dataframes[state_alias] = df\n",
    "                                break\n",
    "            else:\n",
    "                conversion_stats['failed'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            conversion_stats['failed'] += 1\n",
    "            print(f\"   ❌ Error converting {key}: {e}\")\n",
    "    \n",
    "    print(f\"\\n✅ Conversion complete:\")\n",
    "    print(f\"   • Successful: {conversion_stats['success']}\")\n",
    "    print(f\"   • Failed: {conversion_stats['failed']}\")\n",
    "    print(f\"   • Total DataFrames: {len(dataframes)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No data to convert\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. DISPLAY RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83D\uDCCA FETCHING RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if summary:\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    \n",
    "    # Summary by state\n",
    "    print(\"\\n\uD83D\uDDFA️ DATA BY STATE:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    state_stats = summary_df[summary_df['status'] == 'SUCCESS'].groupby('state').agg({\n",
    "        'data_type': 'nunique',\n",
    "        'records': 'sum',\n",
    "        'ba_code': lambda x: ', '.join(sorted(set(x)))\n",
    "    }).reset_index()\n",
    "    \n",
    "    state_stats.columns = ['State', 'Data Types', 'Total Records', 'BA Codes']\n",
    "    print(state_stats.to_string(index=False))\n",
    "    \n",
    "    # Summary by data type\n",
    "    print(\"\\n⚡ DATA BY TYPE:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    type_stats = summary_df[summary_df['status'] == 'SUCCESS'].groupby('data_type').agg({\n",
    "        'state': 'nunique',\n",
    "        'records': 'sum',\n",
    "        'dataset': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    type_stats.columns = ['Data Type', 'States', 'Total Records', 'Datasets']\n",
    "    print(type_stats.to_string(index=False))\n",
    "    \n",
    "    # Overall totals\n",
    "    total_success = len(summary_df[summary_df['status'] == 'SUCCESS'])\n",
    "    total_records = summary_df[summary_df['status'] == 'SUCCESS']['records'].sum()\n",
    "    \n",
    "    print(f\"\\n\uD83D\uDCC8 OVERALL TOTALS:\")\n",
    "    print(f\"   • Total fetch attempts: {len(summary_df)}\")\n",
    "    print(f\"   • Successful fetches: {total_success}\")\n",
    "    print(f\"   • Total records: {total_records:,}\")\n",
    "    print(f\"   • States with data: {state_stats.shape[0]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n❌ No summary data available\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. SAMPLE DATA PREVIEW\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83D\uDC41️ SAMPLE DATA PREVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if dataframes:\n",
    "    # Show samples from different states\n",
    "    sample_states = ['California', 'Texas', 'Florida', 'New York', 'Illinois']\n",
    "    \n",
    "    for state in sample_states:\n",
    "        state_key = state.replace(' ', '_')\n",
    "        state_dfs = [k for k in dataframes.keys() if state_key in k]\n",
    "        \n",
    "        if state_dfs:\n",
    "            print(f\"\\n\uD83C\uDFDB️ {state.upper()}:\")\n",
    "            print(f\"   Available datasets: {len(state_dfs)}\")\n",
    "            \n",
    "            # Show first dataset as sample\n",
    "            sample_key = state_dfs[0]\n",
    "            df = dataframes[sample_key]\n",
    "            \n",
    "            print(f\"\\n   \uD83D\uDCCA {sample_key}:\")\n",
    "            print(f\"      Shape: {df.shape}\")\n",
    "            \n",
    "            if 'period' in df.columns and len(df) > 0:\n",
    "                print(f\"      Time range: {df['period'].iloc[0]} to {df['period'].iloc[-1]}\")\n",
    "            \n",
    "            if 'value' in df.columns and len(df) > 0:\n",
    "                values = pd.to_numeric(df['value'], errors='coerce')\n",
    "                valid_values = values.dropna()\n",
    "                if len(valid_values) > 0:\n",
    "                    print(f\"      Value range: {valid_values.min():.0f} to {valid_values.max():.0f} MWh\")\n",
    "                    print(f\"      Average: {valid_values.mean():.0f} MWh\")\n",
    "            \n",
    "            # Show first 2 rows\n",
    "            print(f\"\\n      First 2 rows:\")\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.width', 1000)\n",
    "            print(df.head(2).to_string())\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\n\uD83C\uDFDB️ {state.upper()}: No data available\")\n",
    "    \n",
    "    # List all available DataFrames\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"\uD83D\uDCCB ALL AVAILABLE DATAFRAMES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nTotal DataFrames: {len(dataframes)}\")\n",
    "    print(\"\\nFirst 20 DataFrames:\")\n",
    "    for i, key in enumerate(sorted(list(dataframes.keys()))[:20], 1):\n",
    "        df = dataframes[key]\n",
    "        print(f\"{i:3}. {key:50} | {df.shape[0]:6} records\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n❌ No DataFrames available\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. HOW TO ACCESS YOUR DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83D\uDE80 HOW TO ACCESS YOUR DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "ACCESS EXAMPLES:\n",
    "----------------\n",
    "\n",
    "1. By State and Data Type (Recommended):\n",
    "   -------------------------------------\n",
    "   # Texas electricity demand\n",
    "   df = dataframes['electricity_demand_Texas_ERCO']\n",
    "   \n",
    "   # California natural gas generation  \n",
    "   df = dataframes['natural_gas_generation_California_CAL']\n",
    "   \n",
    "   # Florida wind generation\n",
    "   df = dataframes['wind_generation_Florida_FPL']\n",
    "   \n",
    "   # Illinois coal generation\n",
    "   df = dataframes['coal_generation_Illinois_MISO']\n",
    "\n",
    "2. By BA Code Only:\n",
    "   -----------------\n",
    "   df = dataframes['electricity_demand_ERCO']\n",
    "   df = dataframes['coal_generation_PJM']\n",
    "   df = dataframes['wind_generation_MISO']\n",
    "\n",
    "3. Check All Available:\n",
    "   ---------------------\n",
    "   print(\"Available DataFrames:\", len(dataframes))\n",
    "   print(\"Keys starting with 'Texas':\")\n",
    "   texas_keys = [k for k in dataframes.keys() if 'Texas' in k]\n",
    "   for key in texas_keys:\n",
    "       print(f\"  • {key}\")\n",
    "\n",
    "QUICK ANALYSIS EXAMPLES:\n",
    "------------------------\n",
    "# 1. Get Texas demand statistics\n",
    "if 'electricity_demand_Texas_ERCO' in dataframes:\n",
    "    df = dataframes['electricity_demand_Texas_ERCO']\n",
    "    df['value_numeric'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    print(\"Texas Demand Stats:\")\n",
    "    print(f\"  Min: {df['value_numeric'].min():.0f} MWh\")\n",
    "    print(f\"  Max: {df['value_numeric'].max():.0f} MWh\") \n",
    "    print(f\"  Mean: {df['value_numeric'].mean():.0f} MWh\")\n",
    "\n",
    "# 2. Compare states\n",
    "states = ['California', 'Texas', 'Florida']\n",
    "for state in states:\n",
    "    key = f'electricity_demand_{state.replace(\" \", \"_\")}'\n",
    "    matching_keys = [k for k in dataframes.keys() if key in k]\n",
    "    if matching_keys:\n",
    "        df = dataframes[matching_keys[0]]\n",
    "        avg_demand = pd.to_numeric(df['value'], errors='coerce').mean()\n",
    "        print(f\"{state}: {avg_demand:.0f} MWh average demand\")\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# 10. FINAL STATUS AND NEXT STEPS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ IMPORT COMPLETE - NEXT STEPS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if dataframes:\n",
    "    print(f\"\"\"\n",
    "\uD83C\uDF89 SUCCESS! Your electricity data has been imported.\n",
    "\n",
    "\uD83D\uDCCA WHAT YOU HAVE:\n",
    "   • {len(dataframes)} DataFrames ready for analysis\n",
    "   • Data for {len(state_stats) if 'state_stats' in locals() else 'multiple'} states\n",
    "   • Hourly data from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\n",
    "   • Four data types: Demand, Coal, Natural Gas, Wind\n",
    "\n",
    "\uD83D\uDD27 TO GET MORE DATA:\n",
    "   1. Increase date range (up to 5000 records limit):\n",
    "      start_date = end_date - timedelta(days=208)  # Max ~208 days\n",
    "   \n",
    "   2. Try additional BA codes:\n",
    "      # Add these to state_ba_mapping\n",
    "      \"New York\": [\"NY\", \"NYIS\", \"PJM\"],\n",
    "      \"Pennsylvania\": [\"PJM\", \"FE\", \"NY\"],\n",
    "      \"Virginia\": [\"PJM\", \"NY\"]\n",
    "\n",
    "\uD83D\uDCC8 READY FOR ANALYSIS:\n",
    "   Start with: dataframes['electricity_demand_California_CAL'].head()\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\"\"\n",
    "⚠️  LIMITED OR NO DATA IMPORTED\n",
    "   \n",
    "   Suggestions:\n",
    "   1. Check which BA codes actually returned data\n",
    "   2. Try with known working codes: CAL, ERCO, SOCO, FE, FPL\n",
    "   3. Reduce date range to 5-7 days for testing\n",
    "   4. Check API response for error messages\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83E\uDDEA QUICK TEST - RUN THESE COMMANDS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "# Test 1: Check what data we have\n",
    "print(f\"Total DataFrames: {len(dataframes)}\")\n",
    "\n",
    "# Test 2: List all Texas data\n",
    "texas_data = [k for k in dataframes.keys() if 'Texas' in k]\n",
    "print(f\"Texas datasets: {len(texas_data)}\")\n",
    "for key in texas_data[:5]:\n",
    "    print(f\"  • {key}\")\n",
    "\n",
    "# Test 3: View a sample DataFrame\n",
    "if dataframes:\n",
    "    sample_key = list(dataframes.keys())[0]\n",
    "    print(f\"\\nSample: {sample_key}\")\n",
    "    print(dataframes[sample_key].head(2))\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ ALL DONE! YOUR DATA IS READY FOR ANALYSIS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9b2668d7-5424-4a61-b635-7afcf4e12ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d74b9043-4b5c-4378-846b-f28286464144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Resumo da Importação de Dados de Energia Elétrica - API EIA\n",
    "\uD83D\uDCCA Visão Geral da Importação\n",
    "Período Analisado: 21 de novembro a 21 de dezembro de 2025 (30 dias)\n",
    "\n",
    "Estados Coletados: 8 dos 10 estados-alvo\n",
    "\n",
    "✅ Sucesso: Califórnia, Texas, Flórida, Geórgia, Illinois, Nova York, Carolina do Norte, Ohio\n",
    "\n",
    "⚠️ Faltantes: Pensilvânia, Virgínia\n",
    "\n",
    "\uD83D\uDCC8 Dados Coletados\n",
    "Total Geral:\n",
    "30.931 registros de dados horários\n",
    "\n",
    "129 DataFrames criados\n",
    "\n",
    "86 datasets importados da API\n",
    "\n",
    "43 de 48 tentativas bem-sucedidas (90% de sucesso)\n",
    "\n",
    "Dados por Tipo de Energia:\n",
    "Demanda Elétrica: 8.652 registros (12 datasets)\n",
    "\n",
    "Geração a Gás Natural: 8.628 registros (12 datasets)\n",
    "\n",
    "Geração a Carvão: 7.907 registros (11 datasets)\n",
    "\n",
    "Geração Eólica: 5.744 registros (8 datasets)\n",
    "\n",
    "Distribuição por Estado:\n",
    "Califórnia e Nova York: 5.768 registros cada (dados mais completos)\n",
    "\n",
    "Carolina do Norte: 4.326 registros\n",
    "\n",
    "Flórida: 3.605 registros\n",
    "\n",
    "Geórgia, Illinois, Ohio: ~2.884 registros cada\n",
    "\n",
    "Texas: 2.812 registros (registros parcialmente completos)\n",
    "\n",
    "⚡ Cobertura de Dados por Estado\n",
    "✅ Estados com Dados Completos (4 tipos):\n",
    "Califórnia (CAL, CISO) - Modelo de \"Duck Curve\"\n",
    "\n",
    "Texas (ERCO) - Maior participação eólica\n",
    "\n",
    "Nova York (NY, NYIS) - Zero carvão na matriz\n",
    "\n",
    "Illinois (MISO) - Maior demanda média\n",
    "\n",
    "Geórgia (SOCO) - Significativa geração a carvão\n",
    "\n",
    "Ohio (PJM) - Maior geração a carvão\n",
    "\n",
    "⚠️ Estados com Dados Parciais:\n",
    "Carolina do Norte: Sem dados de geração eólica\n",
    "\n",
    "Flórida: FPL sem dados de carvão e eólica\n",
    "\n",
    "\uD83D\uDD0D Insights Iniciais dos Dados\n",
    "Padrões de Demanda Identificados:\n",
    "Califórnia: Vale de demanda às 14-15h (efeito solar extremo)\n",
    "\n",
    "Texas: Maior geração eólica (até 16.328 MWh)\n",
    "\n",
    "Illinois: Maior demanda média (77.029 MWh)\n",
    "\n",
    "Nova York/Califórnia: Geração zero ou mínima de carvão\n",
    "\n",
    "Faixas de Valores por Estado:\n",
    "Demanda Máxima: Illinois (99.092 MWh)\n",
    "\n",
    "Demanda Mínima: Flórida-FPC (3.756 MWh)\n",
    "\n",
    "Geração Carvão Máxima: Illinois (24.222 MWh)\n",
    "\n",
    "Geração Eólica Máxima: Texas (16.328 MWh)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d40c2543-c758-4135-ba05-743d8f9e0a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 3: BRONZE LAYER SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef33c51e-a841-4e71-80f6-e8d021991050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A Bronze Layer foi criada como o repositório inicial dos dados brutos da API da EIA, preservando a forma original de 30.931 registros horários de demanda e geração elétrica (carvão, gás natural e eólica) para 8 estados norte-americanos no período de novembro a dezembro de 2025. Esta camada mantém a integridade dos dados originais sem transformações, servindo como fonte única de verdade para auditoria e reprocessamento futuro, enquanto fornece a base para as transformações subsequentes nas camadas Silver e Gold do pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b281feed-b6b8-4857-a2ce-e8eeeaa20c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n\uD83E\uDD49 BRONZE LAYER - DATABRICKS COMMUNITY EDITION\n================================================================================\n\n\uD83D\uDD27 PREPARING DATA FOR BRONZE LAYER\n\n\uD83D\uDCE5 PROCESSING DATAFRAMES INTO BRONZE LAYER\n\nTotal DataFrames to process: 129\n\n\uD83D\uDCCA DataFrames by type:\n  • DEMAND    : 36 DataFrames\n  • COAL      : 33 DataFrames\n  • GAS       : 36 DataFrames\n  • WIND      : 24 DataFrames\n\n================================================================================\n\uD83D\uDD04 CREATING BRONZE VIEWS\n================================================================================\n\n\uD83D\uDCC2 Processing DEMAND DataFrames (36):\n  ✅ electricity_demand_CISO                            -    721 records\n  ✅ electricity_California_CISO                        -    721 records\n  ✅ electricity_demand_California_CISO                 -    721 records\n  ✅ electricity_demand_FPC                             -    721 records\n  ✅ electricity_Florida_FPC                            -    721 records\n  ✅ electricity_demand_Florida_FPC                     -    721 records\n  ✅ electricity_demand_PJM                             -    721 records\n  ✅ electricity_Ohio_PJM                               -    721 records\n  ✅ electricity_demand_Ohio_PJM                        -    721 records\n  ✅ electricity_demand_NYIS                            -    721 records\n  ✅ electricity_New_York_NYIS                          -    721 records\n  ✅ electricity_demand_New_York_NYIS                   -    721 records\n  ✅ electricity_demand_SOCO                            -    721 records\n  ✅ electricity_Georgia_SOCO                           -    721 records\n  ✅ electricity_demand_Georgia_SOCO                    -    721 records\n  ✅ electricity_demand_DUK                             -    721 records\n  ✅ electricity_North_Carolina_DUK                     -    721 records\n  ✅ electricity_demand_North_Carolina_DUK              -    721 records\n  ✅ electricity_demand_FPL                             -    721 records\n  ✅ electricity_Florida_FPL                            -    721 records\n  ✅ electricity_demand_Florida_FPL                     -    721 records\n  ✅ electricity_demand_CAR                             -    721 records\n  ✅ electricity_North_Carolina_CAR                     -    721 records\n  ✅ electricity_demand_North_Carolina_CAR              -    721 records\n  ✅ electricity_demand_ERCO                            -    721 records\n  ✅ electricity_Texas_ERCO                             -    721 records\n  ✅ electricity_demand_Texas_ERCO                      -    721 records\n  ✅ electricity_demand_CAL                             -    721 records\n  ✅ electricity_California_CAL                         -    721 records\n  ✅ electricity_demand_California_CAL                  -    721 records\n  ✅ electricity_demand_NY                              -    721 records\n  ✅ electricity_New_York_NY                            -    721 records\n  ✅ electricity_demand_New_York_NY                     -    721 records\n  ✅ electricity_demand_MISO                            -    721 records\n  ✅ electricity_Illinois_MISO                          -    721 records\n  ✅ electricity_demand_Illinois_MISO                   -    721 records\n\n  \uD83C\uDFAF Created view: bronze_demand\n    Total records: 25,956\n    Columns: period, respondent, respondent_name, type, type_name, value, value_units, data_type...\n    Sample (first 2 rows):\n+-------------+----------+--------------------------------------+----+---------+-----+-------------+---------+----------+-------+--------------------------+-----------------------+--------------------------------------------------+\n|       period|respondent|                       respondent_name|type|type_name|value|  value_units|data_type|     state|ba_code|       ingestion_timestamp|            source_file|                                         record_id|\n+-------------+----------+--------------------------------------+----+---------+-----+-------------+---------+----------+-------+--------------------------+-----------------------+--------------------------------------------------+\n|2025-11-21T00|      CISO|California Independent System Operator|   D|   Demand|26814|megawatthours|   demand|California|   CISO|2025-12-21 17:59:37.408207|electricity_demand_CISO|electricity_demand_CISO_Column<'current_timesta...|\n|2025-11-21T01|      CISO|California Independent System Operator|   D|   Demand|25860|megawatthours|   demand|California|   CISO|2025-12-21 17:59:37.408207|electricity_demand_CISO|electricity_demand_CISO_Column<'current_timesta...|\n+-------------+----------+--------------------------------------+----+---------+-----+-------------+---------+----------+-------+--------------------------+-----------------------+--------------------------------------------------+\n\n\n\uD83D\uDCC2 Processing COAL DataFrames (33):\n  ✅ coal_generation_CISO                               -    721 records\n  ✅ coal_California_CISO                               -    721 records\n  ✅ coal_generation_California_CISO                    -    721 records\n  ✅ coal_generation_FPC                                -    721 records\n  ✅ coal_Florida_FPC                                   -    721 records\n  ✅ coal_generation_Florida_FPC                        -    721 records\n  ✅ coal_generation_PJM                                -    721 records\n  ✅ coal_Ohio_PJM                                      -    721 records\n  ✅ coal_generation_Ohio_PJM                           -    721 records\n  ✅ coal_generation_NYIS                               -    721 records\n  ✅ coal_New_York_NYIS                                 -    721 records\n  ✅ coal_generation_New_York_NYIS                      -    721 records\n  ✅ coal_generation_SOCO                               -    721 records\n  ✅ coal_Georgia_SOCO                                  -    721 records\n  ✅ coal_generation_Georgia_SOCO                       -    721 records\n  ✅ coal_generation_DUK                                -    721 records\n  ✅ coal_North_Carolina_DUK                            -    721 records\n  ✅ coal_generation_North_Carolina_DUK                 -    721 records\n  ✅ coal_generation_CAR                                -    721 records\n  ✅ coal_North_Carolina_CAR                            -    721 records\n  ✅ coal_generation_North_Carolina_CAR                 -    721 records\n  ✅ coal_generation_ERCO                               -    697 records\n  ✅ coal_Texas_ERCO                                    -    697 records\n  ✅ coal_generation_Texas_ERCO                         -    697 records\n  ✅ coal_generation_CAL                                -    721 records\n  ✅ coal_California_CAL                                -    721 records\n  ✅ coal_generation_California_CAL                     -    721 records\n  ✅ coal_generation_NY                                 -    721 records\n  ✅ coal_New_York_NY                                   -    721 records\n  ✅ coal_generation_New_York_NY                        -    721 records\n  ✅ coal_generation_MISO                               -    721 records\n  ✅ coal_Illinois_MISO                                 -    721 records\n  ✅ coal_generation_Illinois_MISO                      -    721 records\n\n  \uD83C\uDFAF Created view: bronze_coal\n    Total records: 23,721\n    Columns: period, respondent, respondent_name, fueltype, type_name, value, value_units, data_type...\n    Sample (first 2 rows):\n+-------------+----------+--------------------------------------+--------+---------+-----+-------------+---------+----------+-------+--------------------------+--------------------+--------------------------------------------------+\n|       period|respondent|                       respondent_name|fueltype|type_name|value|  value_units|data_type|     state|ba_code|       ingestion_timestamp|         source_file|                                         record_id|\n+-------------+----------+--------------------------------------+--------+---------+-----+-------------+---------+----------+-------+--------------------------+--------------------+--------------------------------------------------+\n|2025-11-21T00|      CISO|California Independent System Operator|     COL|     Coal|    0|megawatthours|     coal|California|   CISO|2025-12-21 17:59:53.243959|coal_generation_CISO|coal_generation_CISO_Column<'current_timestamp()'>|\n|2025-11-21T01|      CISO|California Independent System Operator|     COL|     Coal|    0|megawatthours|     coal|California|   CISO|2025-12-21 17:59:53.243959|coal_generation_CISO|coal_generation_CISO_Column<'current_timestamp()'>|\n+-------------+----------+--------------------------------------+--------+---------+-----+-------------+---------+----------+-------+--------------------------+--------------------+--------------------------------------------------+\n\n\n\uD83D\uDCC2 Processing GAS DataFrames (36):\n  ✅ natural_gas_generation_CISO                        -    721 records\n  ✅ natural_California_CISO                            -    721 records\n  ✅ natural_gas_generation_California_CISO             -    721 records\n  ✅ natural_gas_generation_FPC                         -    721 records\n  ✅ natural_Florida_FPC                                -    721 records\n  ✅ natural_gas_generation_Florida_FPC                 -    721 records\n  ✅ natural_gas_generation_PJM                         -    721 records\n  ✅ natural_Ohio_PJM                                   -    721 records\n  ✅ natural_gas_generation_Ohio_PJM                    -    721 records\n  ✅ natural_gas_generation_NYIS                        -    721 records\n  ✅ natural_New_York_NYIS                              -    721 records\n  ✅ natural_gas_generation_New_York_NYIS               -    721 records\n  ✅ natural_gas_generation_SOCO                        -    721 records\n  ✅ natural_Georgia_SOCO                               -    721 records\n  ✅ natural_gas_generation_Georgia_SOCO                -    721 records\n  ✅ natural_gas_generation_DUK                         -    721 records\n  ✅ natural_North_Carolina_DUK                         -    721 records\n  ✅ natural_gas_generation_North_Carolina_DUK          -    721 records\n  ✅ natural_gas_generation_FPL                         -    721 records\n  ✅ natural_Florida_FPL                                -    721 records\n  ✅ natural_gas_generation_Florida_FPL                 -    721 records\n  ✅ natural_gas_generation_CAR                         -    721 records\n  ✅ natural_North_Carolina_CAR                         -    721 records\n  ✅ natural_gas_generation_North_Carolina_CAR          -    721 records\n  ✅ natural_gas_generation_ERCO                        -    697 records\n  ✅ natural_Texas_ERCO                                 -    697 records\n  ✅ natural_gas_generation_Texas_ERCO                  -    697 records\n  ✅ natural_gas_generation_CAL                         -    721 records\n  ✅ natural_California_CAL                             -    721 records\n  ✅ natural_gas_generation_California_CAL              -    721 records\n  ✅ natural_gas_generation_NY                          -    721 records\n  ✅ natural_New_York_NY                                -    721 records\n  ✅ natural_gas_generation_New_York_NY                 -    721 records\n  ✅ natural_gas_generation_MISO                        -    721 records\n  ✅ natural_Illinois_MISO                              -    721 records\n  ✅ natural_gas_generation_Illinois_MISO               -    721 records\n\n  \uD83C\uDFAF Created view: bronze_gas\n    Total records: 25,884\n    Columns: period, respondent, respondent_name, fueltype, type_name, value, value_units, data_type...\n    Sample (first 2 rows):\n+-------------+----------+--------------------------------------+--------+-----------+-----+-------------+---------+-------+-------+--------------------------+---------------------------+--------------------------------------------------+\n|       period|respondent|                       respondent_name|fueltype|  type_name|value|  value_units|data_type|  state|ba_code|       ingestion_timestamp|                source_file|                                         record_id|\n+-------------+----------+--------------------------------------+--------+-----------+-----+-------------+---------+-------+-------+--------------------------+---------------------------+--------------------------------------------------+\n|2025-11-21T00|      CISO|California Independent System Operator|      NG|Natural Gas|12453|megawatthours|      gas|Georgia|   CISO|2025-12-21 18:00:11.376976|natural_gas_generation_CISO|natural_gas_generation_CISO_Column<'current_tim...|\n|2025-11-21T01|      CISO|California Independent System Operator|      NG|Natural Gas|11442|megawatthours|      gas|Georgia|   CISO|2025-12-21 18:00:11.376976|natural_gas_generation_CISO|natural_gas_generation_CISO_Column<'current_tim...|\n+-------------+----------+--------------------------------------+--------+-----------+-----+-------------+---------+-------+-------+--------------------------+---------------------------+--------------------------------------------------+\n\n\n\uD83D\uDCC2 Processing WIND DataFrames (24):\n  ✅ wind_generation_CISO                               -    721 records\n  ✅ wind_California_CISO                               -    721 records\n  ✅ wind_generation_California_CISO                    -    721 records\n  ✅ wind_generation_PJM                                -    721 records\n  ✅ wind_Ohio_PJM                                      -    721 records\n  ✅ wind_generation_Ohio_PJM                           -    721 records\n  ✅ wind_generation_NYIS                               -    721 records\n  ✅ wind_New_York_NYIS                                 -    721 records\n  ✅ wind_generation_New_York_NYIS                      -    721 records\n  ✅ wind_generation_SOCO                               -    721 records\n  ✅ wind_Georgia_SOCO                                  -    721 records\n  ✅ wind_generation_Georgia_SOCO                       -    721 records\n  ✅ wind_generation_ERCO                               -    697 records\n  ✅ wind_Texas_ERCO                                    -    697 records\n  ✅ wind_generation_Texas_ERCO                         -    697 records\n  ✅ wind_generation_CAL                                -    721 records\n  ✅ wind_California_CAL                                -    721 records\n  ✅ wind_generation_California_CAL                     -    721 records\n  ✅ wind_generation_NY                                 -    721 records\n  ✅ wind_New_York_NY                                   -    721 records\n  ✅ wind_generation_New_York_NY                        -    721 records\n  ✅ wind_generation_MISO                               -    721 records\n  ✅ wind_Illinois_MISO                                 -    721 records\n  ✅ wind_generation_Illinois_MISO                      -    721 records\n\n  \uD83C\uDFAF Created view: bronze_wind\n    Total records: 17,232\n    Columns: period, respondent, respondent_name, fueltype, type_name, value, value_units, data_type...\n    Sample (first 2 rows):\n+-------------+----------+--------------------------------------+--------+---------+-----+-------------+---------+----------+-------+--------------------------+--------------------+--------------------------------------------------+\n|       period|respondent|                       respondent_name|fueltype|type_name|value|  value_units|data_type|     state|ba_code|       ingestion_timestamp|         source_file|                                         record_id|\n+-------------+----------+--------------------------------------+--------+---------+-----+-------------+---------+----------+-------+--------------------------+--------------------+--------------------------------------------------+\n|2025-11-21T00|      CISO|California Independent System Operator|     WND|     Wind| 1245|megawatthours|     wind|California|   CISO|2025-12-21 18:00:22.322741|wind_generation_CISO|wind_generation_CISO_Column<'current_timestamp()'>|\n|2025-11-21T01|      CISO|California Independent System Operator|     WND|     Wind| 1232|megawatthours|     wind|California|   CISO|2025-12-21 18:00:22.322741|wind_generation_CISO|wind_generation_CISO_Column<'current_timestamp()'>|\n+-------------+----------+--------------------------------------+--------+---------+-----+-------------+---------+----------+-------+--------------------------+--------------------+--------------------------------------------------+\n\n\n================================================================================\n\uD83C\uDF09 CREATING UNIFIED BRONZE VIEW\n================================================================================\n\n\uD83D\uDCCB Available bronze views: ['bronze_demand', 'bronze_coal', 'bronze_gas', 'bronze_wind']\n\n✅ Created unified view: bronze_all_data\n   Total records: 92,793\n   Combined from: 4 data types\n\n================================================================================\n\uD83D\uDCCA CREATING METADATA VIEW\n================================================================================\n\n✅ Created metadata view: bronze_metadata\n\n\uD83D\uDCCB Metadata content:\n+----------+---------+------------+-------------+\n|created_at|data_type|record_count|view_name    |\n+----------+---------+------------+-------------+\n|{}        |demand   |25956       |bronze_demand|\n|{}        |coal     |23721       |bronze_coal  |\n|{}        |gas      |25884       |bronze_gas   |\n|{}        |wind     |17232       |bronze_wind  |\n+----------+---------+------------+-------------+\n\n\n================================================================================\n\uD83D\uDD0D VERIFYING BRONZE LAYER\n================================================================================\n\n\uD83D\uDCCB ALL BRONZE VIEWS:\n  • bronze_all_data           -   92,793 records\n  • bronze_coal               -   23,721 records\n  • bronze_demand             -   25,956 records\n  • bronze_gas                -   25,884 records\n  • bronze_metadata           -        4 records\n  • bronze_wind               -   17,232 records\n\n================================================================================\n\uD83E\uDDEA DATA QUALITY CHECKS\n================================================================================\n\n\uD83D\uDCC8 DATA QUALITY METRICS:\n\n1. Missing Values Check:\n   • Total records: 92,793\n   • Missing period: 0 (0.0%)\n   • Missing value: 0 (0.0%)\n   • Unknown state: 0 (0.0%)\n\n2. Data Distribution by Type:\n   • demand    : 25,956 records, 8 states, 11 BA codes\n   • gas       : 25,884 records, 8 states, 11 BA codes\n   • coal      : 23,721 records, 8 states, 10 BA codes\n   • wind      : 17,232 records, 6 states, 7 BA codes\n\n3. Value Ranges by Data Type:\n   • demand    : Min=3756, Max=135723, Avg=33909\n   • coal      : Min=-46, Max=31509, Avg=5743\n   • gas       : Min=1465, Max=60177, Avg=13775\n   • wind      : Min=0, Max=27550, Avg=4514\n\n4. Sample Data:\n+---------+----------+-------+-------------+-----+\n|data_type|state     |ba_code|period       |value|\n+---------+----------+-------+-------------+-----+\n|demand   |Florida   |FPC    |2025-12-20T00|6531 |\n|demand   |California|CISO   |2025-12-20T00|26385|\n|demand   |California|CISO   |2025-12-20T00|26385|\n|demand   |California|CISO   |2025-12-20T00|26385|\n|demand   |Florida   |FPC    |2025-12-20T00|6531 |\n+---------+----------+-------+-------------+-----+\n\n\n================================================================================\n✅ BRONZE LAYER COMPLETE!\n================================================================================\n\n\uD83C\uDF89 BRONZE LAYER SUCCESSFULLY CREATED!\n\n\uD83D\uDCCA INGESTION SUMMARY:\n   • Processed: 129 DataFrames successfully\n   • Failed: 0 DataFrames\n   • Created 4 bronze views\n   • Total records in unified view: 92,793\n\n\uD83D\uDCCB AVAILABLE VIEWS:\n   • bronze_all_data      - All unified data\n   • bronze_metadata      - Metadata about bronze layer\n   • bronze_demand        - Electricity demand data\n   • bronze_coal          - Coal generation data  \n   • bronze_gas           - Natural gas generation data\n   • bronze_wind          - Wind generation data\n\n\uD83D\uDD0D HOW TO USE YOUR BRONZE DATA:\n\n1. Query specific data type:\n   display(spark.sql('SELECT * FROM bronze_demand LIMIT 5'))\n\n2. Analyze by state:\n   display(spark.sql(\n        \"SELECT state, COUNT(*) as records \"\n        \"FROM bronze_all_data \"\n        \"GROUP BY state \"\n        \"ORDER BY records DESC\"\n   ))\n\n3. Check data quality:\n   display(spark.sql(\n        \"SELECT \"\n        \"    data_type, \"\n        \"    COUNT(*) as total, \"\n        \"    SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END) as null_values \"\n        \"FROM bronze_all_data \"\n        \"GROUP BY data_type\"\n   ))\n\n4. Time series analysis:\n   display(spark.sql(\n        \"SELECT \"\n        \"    DATE(period) as date, \"\n        \"    AVG(CAST(value AS DOUBLE)) as avg_value \"\n        \"FROM bronze_all_data \"\n        \"WHERE data_type = 'demand' \"\n        \"GROUP BY DATE(period) \"\n        \"ORDER BY date\"\n   ))\n\n\uD83E\uDDEA QUICK TEST QUERIES:\n\nTest 1: Count by data type\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>data_type</th><th>count</th></tr></thead><tbody><tr><td>demand</td><td>25956</td></tr><tr><td>gas</td><td>25884</td></tr><tr><td>coal</td><td>23721</td></tr><tr><td>wind</td><td>17232</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "demand",
         25956
        ],
        [
         "gas",
         25884
        ],
        [
         "coal",
         23721
        ],
        [
         "wind",
         17232
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTest 2: States with most data\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>state</th><th>records</th></tr></thead><tbody><tr><td>New York</td><td>17304</td></tr><tr><td>Georgia</td><td>17256</td></tr><tr><td>California</td><td>14420</td></tr><tr><td>North Carolina</td><td>12978</td></tr><tr><td>Illinois</td><td>8652</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "New York",
         17304
        ],
        [
         "Georgia",
         17256
        ],
        [
         "California",
         14420
        ],
        [
         "North Carolina",
         12978
        ],
        [
         "Illinois",
         8652
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "records",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTest 3: Latest data sample\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>data_type</th><th>state</th><th>period</th><th>value</th></tr></thead><tbody><tr><td>demand</td><td>California</td><td>2025-12-20T23</td><td>25454</td></tr><tr><td>demand</td><td>California</td><td>2025-12-20T23</td><td>25454</td></tr><tr><td>demand</td><td>California</td><td>2025-12-20T23</td><td>25454</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "demand",
         "California",
         "2025-12-20T23",
         "25454"
        ],
        [
         "demand",
         "California",
         "2025-12-20T23",
         "25454"
        ],
        [
         "demand",
         "California",
         "2025-12-20T23",
         "25454"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "period",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n\uD83D\uDE80 READY FOR SILVER LAYER PROCESSING!\n================================================================================\n\nYour bronze layer is now available as temporary views.\nProceed to create the Silver Layer for data cleaning and transformation.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BRONZE LAYER INGESTION FOR COMMUNITY EDITION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83E\uDD49 BRONZE LAYER - DATABRICKS COMMUNITY EDITION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PREPARE HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n\uD83D\uDD27 PREPARING DATA FOR BRONZE LAYER\")\n",
    "\n",
    "def extract_metadata_from_key(key):\n",
    "    \"\"\"Extract state, data_type, and BA code from DataFrame key\"\"\"\n",
    "    key_lower = key.lower()\n",
    "    \n",
    "    # Extract data type\n",
    "    if 'demand' in key_lower or 'electricity' in key_lower:\n",
    "        data_type = 'demand'\n",
    "    elif 'coal' in key_lower:\n",
    "        data_type = 'coal'\n",
    "    elif 'gas' in key_lower or 'natural' in key_lower:\n",
    "        data_type = 'gas'\n",
    "    elif 'wind' in key_lower:\n",
    "        data_type = 'wind'\n",
    "    else:\n",
    "        data_type = 'unknown'\n",
    "    \n",
    "    # Extract state\n",
    "    state = 'Unknown'\n",
    "    states = {\n",
    "        'California': ['california', 'cal', 'ciso'],\n",
    "        'Texas': ['texas', 'erco'],\n",
    "        'Florida': ['florida', 'fl', 'fpl', 'fpc'],\n",
    "        'Ohio': ['ohio', 'oh', 'pjm'],\n",
    "        'Georgia': ['georgia', 'ga', 'soco'],\n",
    "        'New_York': ['new_york', 'ny', 'nyis'],\n",
    "        'North_Carolina': ['north_carolina', 'nc', 'car', 'duk'],\n",
    "        'Illinois': ['illinois', 'il', 'miso'],\n",
    "        'Pennsylvania': ['pennsylvania', 'pa'],\n",
    "        'Virginia': ['virginia', 'va']\n",
    "    }\n",
    "    \n",
    "    for state_name, keywords in states.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in key_lower:\n",
    "                state = state_name.replace('_', ' ')\n",
    "                break\n",
    "    \n",
    "    # Extract BA code\n",
    "    ba_code = 'Unknown'\n",
    "    ba_codes = ['ERCO', 'PJM', 'MISO', 'CISO', 'CAL', 'NY', 'NYIS', 'SOCO', 'DUK', 'CAR', 'FPL', 'FPC', 'TECO']\n",
    "    for code in ba_codes:\n",
    "        if code in key.upper():\n",
    "            ba_code = code\n",
    "            break\n",
    "    \n",
    "    return state, data_type, ba_code\n",
    "\n",
    "# ============================================================================\n",
    "# 2. PROCESS EACH DATAFRAME\n",
    "# ============================================================================\n",
    "print(\"\\n\uD83D\uDCE5 PROCESSING DATAFRAMES INTO BRONZE LAYER\")\n",
    "\n",
    "bronze_views = {}\n",
    "processed_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "print(f\"\\nTotal DataFrames to process: {len(dataframes)}\")\n",
    "\n",
    "# Group DataFrames by type for organized processing\n",
    "dataframes_by_type = {'demand': [], 'coal': [], 'gas': [], 'wind': []}\n",
    "\n",
    "for df_key in dataframes.keys():\n",
    "    state, data_type, ba_code = extract_metadata_from_key(df_key)\n",
    "    if data_type in dataframes_by_type:\n",
    "        dataframes_by_type[data_type].append(df_key)\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA DataFrames by type:\")\n",
    "for data_type, df_list in dataframes_by_type.items():\n",
    "    print(f\"  • {data_type.upper():10}: {len(df_list)} DataFrames\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CREATE BRONZE VIEWS BY DATA TYPE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83D\uDD04 CREATING BRONZE VIEWS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Process each data type separately\n",
    "for data_type, df_keys in dataframes_by_type.items():\n",
    "    if not df_keys:\n",
    "        print(f\"\\n\uD83D\uDCED No DataFrames found for {data_type}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n\uD83D\uDCC2 Processing {data_type.upper()} DataFrames ({len(df_keys)}):\")\n",
    "    \n",
    "    # List to hold processed DataFrames of this type\n",
    "    processed_dfs = []\n",
    "    \n",
    "    for df_key in df_keys:\n",
    "        try:\n",
    "                      # Get the DataFrame (might be pandas or Spark)\n",
    "            df = dataframes[df_key]\n",
    "            \n",
    "            # Convert pandas DataFrame to Spark DataFrame if needed\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                spark_df = spark.createDataFrame(df)\n",
    "            else:\n",
    "                spark_df = df  # Assume it's already a Spark DataFrame\n",
    "            \n",
    "            # Extract metadata\n",
    "            state, data_type_from_key, ba_code = extract_metadata_from_key(df_key)\n",
    "            \n",
    "            # Add metadata columns\n",
    "            bronze_df = spark_df.withColumn(\"data_type\", lit(data_type_from_key)) \\\n",
    "                                .withColumn(\"state\", lit(state)) \\\n",
    "                                .withColumn(\"ba_code\", lit(ba_code)) \\\n",
    "                                .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "                                .withColumn(\"source_file\", lit(df_key)) \\\n",
    "                                .withColumn(\"record_id\", lit(f\"{df_key}_{current_timestamp()}\"))\n",
    "            \n",
    "            # Standardize column names if needed\n",
    "            column_mapping = {\n",
    "                'respondent-name': 'respondent_name',\n",
    "                'type-name': 'type_name', \n",
    "                'value-units': 'value_units'\n",
    "            }\n",
    "            \n",
    "            for old_col, new_col in column_mapping.items():\n",
    "                if old_col in bronze_df.columns:\n",
    "                    bronze_df = bronze_df.withColumnRenamed(old_col, new_col)\n",
    "            \n",
    "            processed_dfs.append(bronze_df)\n",
    "            print(f\"  ✅ {df_key:50} - {spark_df.count():6} records\")\n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ {df_key:50} - ERROR: {str(e)[:80]}\")\n",
    "            failed_count += 1\n",
    "    \n",
    "    # Create unified view for this data type\n",
    "    if processed_dfs:\n",
    "        try:\n",
    "            # Union all DataFrames of this type\n",
    "            from functools import reduce\n",
    "            from pyspark.sql import DataFrame\n",
    "            \n",
    "            # Start with first DataFrame\n",
    "            unified_df = processed_dfs[0]\n",
    "            \n",
    "            # Union with remaining DataFrames\n",
    "            for df in processed_dfs[1:]:\n",
    "                unified_df = unified_df.unionByName(df, allowMissingColumns=True)\n",
    "            \n",
    "            # Create temporary view\n",
    "            view_name = f\"bronze_{data_type}\"\n",
    "            unified_df.createOrReplaceTempView(view_name)\n",
    "            bronze_views[view_name] = unified_df.count()\n",
    "            \n",
    "            print(f\"\\n  \uD83C\uDFAF Created view: {view_name}\")\n",
    "            print(f\"    Total records: {unified_df.count():,}\")\n",
    "            print(f\"    Columns: {', '.join(unified_df.columns[:8])}...\")\n",
    "            \n",
    "            # Show sample\n",
    "            print(f\"    Sample (first 2 rows):\")\n",
    "            unified_df.limit(2).show(truncate=50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Failed to create {data_type} view: {str(e)[:100]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CREATE UNIFIED BRONZE VIEW\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83C\uDF09 CREATING UNIFIED BRONZE VIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all bronze views\n",
    "bronze_view_names = [f\"bronze_{dt}\" for dt in ['demand', 'coal', 'gas', 'wind']]\n",
    "\n",
    "# Check which views exist\n",
    "existing_views = []\n",
    "for view_name in bronze_view_names:\n",
    "    try:\n",
    "        spark.sql(f\"SELECT 1 FROM {view_name} LIMIT 1\")\n",
    "        existing_views.append(view_name)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if existing_views:\n",
    "    print(f\"\\n\uD83D\uDCCB Available bronze views: {existing_views}\")\n",
    "    \n",
    "    # Build UNION query\n",
    "    union_parts = []\n",
    "    for view_name in existing_views:\n",
    "        union_parts.append(f\"SELECT * FROM {view_name}\")\n",
    "    \n",
    "    if union_parts:\n",
    "        union_query = \" UNION ALL \".join(union_parts)\n",
    "        \n",
    "        # Create unified view\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW bronze_all_data AS\n",
    "            {union_query}\n",
    "        \"\"\")\n",
    "        \n",
    "        # Get count\n",
    "        total_records = spark.sql(\"SELECT COUNT(*) as total FROM bronze_all_data\").collect()[0]['total']\n",
    "        \n",
    "        print(f\"\\n✅ Created unified view: bronze_all_data\")\n",
    "        print(f\"   Total records: {total_records:,}\")\n",
    "        print(f\"   Combined from: {len(existing_views)} data types\")\n",
    "else:\n",
    "    print(\"\\n❌ No bronze views available to unify\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CREATE METADATA VIEW\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83D\uDCCA CREATING METADATA VIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create metadata DataFrame\n",
    "metadata_data = []\n",
    "for view_name, count in bronze_views.items():\n",
    "    metadata_data.append({\n",
    "        'view_name': view_name,\n",
    "        'record_count': count,\n",
    "        'data_type': view_name.replace('bronze_', ''),\n",
    "        'created_at': pd.Timestamp.now()\n",
    "    })\n",
    "\n",
    "if metadata_data:\n",
    "    metadata_df = spark.createDataFrame(metadata_data)\n",
    "    metadata_df.createOrReplaceTempView(\"bronze_metadata\")\n",
    "    \n",
    "    print(f\"\\n✅ Created metadata view: bronze_metadata\")\n",
    "    print(\"\\n\uD83D\uDCCB Metadata content:\")\n",
    "    metadata_df.show(truncate=False)\n",
    "else:\n",
    "    print(\"\\n❌ No metadata to create\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. VERIFY AND SUMMARIZE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83D\uDD0D VERIFYING BRONZE LAYER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# List all available bronze views\n",
    "print(\"\\n\uD83D\uDCCB ALL BRONZE VIEWS:\")\n",
    "all_views = [v for v in spark.catalog.listTables() if v.name.startswith('bronze_')]\n",
    "for view in all_views:\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {view.name}\").collect()[0]['cnt']\n",
    "        print(f\"  • {view.name:25} - {count:8,} records\")\n",
    "    except:\n",
    "        print(f\"  • {view.name:25} - ERROR\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. RUN DATA QUALITY CHECKS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83E\uDDEA DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'bronze_all_data' in [v.name for v in spark.catalog.listTables()]:\n",
    "    print(\"\\n\uD83D\uDCC8 DATA QUALITY METRICS:\")\n",
    "    \n",
    "    # 1. Check for missing values\n",
    "    print(\"\\n1. Missing Values Check:\")\n",
    "    result = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_records,\n",
    "            SUM(CASE WHEN period IS NULL THEN 1 ELSE 0 END) as missing_period,\n",
    "            SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END) as missing_value,\n",
    "            SUM(CASE WHEN state = 'Unknown' THEN 1 ELSE 0 END) as unknown_state\n",
    "        FROM bronze_all_data\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"   • Total records: {result['total_records']:,}\")\n",
    "    print(f\"   • Missing period: {result['missing_period']:,} ({result['missing_period']/result['total_records']*100:.1f}%)\")\n",
    "    print(f\"   • Missing value: {result['missing_value']:,} ({result['missing_value']/result['total_records']*100:.1f}%)\")\n",
    "    print(f\"   • Unknown state: {result['unknown_state']:,} ({result['unknown_state']/result['total_records']*100:.1f}%)\")\n",
    "    \n",
    "    # 2. Check data distribution\n",
    "    print(\"\\n2. Data Distribution by Type:\")\n",
    "    dist_result = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            data_type,\n",
    "            COUNT(*) as record_count,\n",
    "            COUNT(DISTINCT state) as state_count,\n",
    "            COUNT(DISTINCT ba_code) as ba_count,\n",
    "            MIN(period) as earliest,\n",
    "            MAX(period) as latest\n",
    "        FROM bronze_all_data\n",
    "        GROUP BY data_type\n",
    "        ORDER BY record_count DESC\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    for row in dist_result:\n",
    "        print(f\"   • {row['data_type']:10}: {row['record_count']:6,} records, {row['state_count']} states, {row['ba_count']} BA codes\")\n",
    "    \n",
    "    # 3. Check value ranges\n",
    "    print(\"\\n3. Value Ranges by Data Type:\")\n",
    "    value_ranges = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            data_type,\n",
    "            MIN(CAST(value AS DOUBLE)) as min_value,\n",
    "            MAX(CAST(value AS DOUBLE)) as max_value,\n",
    "            AVG(CAST(value AS DOUBLE)) as avg_value\n",
    "        FROM bronze_all_data\n",
    "        WHERE value IS NOT NULL AND value != ''\n",
    "        GROUP BY data_type\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    for row in value_ranges:\n",
    "        print(f\"   • {row['data_type']:10}: Min={row['min_value']:.0f}, Max={row['max_value']:.0f}, Avg={row['avg_value']:.0f}\")\n",
    "    \n",
    "    # 4. Show sample queries\n",
    "    print(\"\\n4. Sample Data:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            data_type,\n",
    "            state,\n",
    "            ba_code,\n",
    "            period,\n",
    "            value\n",
    "        FROM bronze_all_data\n",
    "        WHERE period LIKE '2025-12-20%'\n",
    "        ORDER BY period\n",
    "        LIMIT 5\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "# ============================================================================\n",
    "# # ============================================================================\n",
    "# 8. FINAL SUMMARY AND USAGE GUIDE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ BRONZE LAYER COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f'''\n",
    "\uD83C\uDF89 BRONZE LAYER SUCCESSFULLY CREATED!\n",
    "\n",
    "\uD83D\uDCCA INGESTION SUMMARY:\n",
    "   • Processed: {processed_count} DataFrames successfully\n",
    "   • Failed: {failed_count} DataFrames\n",
    "   • Created {len(bronze_views)} bronze views\n",
    "   • Total records in unified view: {total_records if 'total_records' in locals() else 0:,}\n",
    "\n",
    "\uD83D\uDCCB AVAILABLE VIEWS:\n",
    "   • bronze_all_data      - All unified data\n",
    "   • bronze_metadata      - Metadata about bronze layer\n",
    "   • bronze_demand        - Electricity demand data\n",
    "   • bronze_coal          - Coal generation data  \n",
    "   • bronze_gas           - Natural gas generation data\n",
    "   • bronze_wind          - Wind generation data\n",
    "\n",
    "\uD83D\uDD0D HOW TO USE YOUR BRONZE DATA:\n",
    "\n",
    "1. Query specific data type:\n",
    "   display(spark.sql('SELECT * FROM bronze_demand LIMIT 5'))\n",
    "\n",
    "2. Analyze by state:\n",
    "   display(spark.sql(\n",
    "        \"SELECT state, COUNT(*) as records \"\n",
    "        \"FROM bronze_all_data \"\n",
    "        \"GROUP BY state \"\n",
    "        \"ORDER BY records DESC\"\n",
    "   ))\n",
    "\n",
    "3. Check data quality:\n",
    "   display(spark.sql(\n",
    "        \"SELECT \"\n",
    "        \"    data_type, \"\n",
    "        \"    COUNT(*) as total, \"\n",
    "        \"    SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END) as null_values \"\n",
    "        \"FROM bronze_all_data \"\n",
    "        \"GROUP BY data_type\"\n",
    "   ))\n",
    "\n",
    "4. Time series analysis:\n",
    "   display(spark.sql(\n",
    "        \"SELECT \"\n",
    "        \"    DATE(period) as date, \"\n",
    "        \"    AVG(CAST(value AS DOUBLE)) as avg_value \"\n",
    "        \"FROM bronze_all_data \"\n",
    "        \"WHERE data_type = 'demand' \"\n",
    "        \"GROUP BY DATE(period) \"\n",
    "        \"ORDER BY date\"\n",
    "   ))\n",
    "\n",
    "\uD83E\uDDEA QUICK TEST QUERIES:\n",
    "''')\n",
    "\n",
    "# Run a quick test\n",
    "try:\n",
    "    print(\"Test 1: Count by data type\")\n",
    "    display(\n",
    "        spark.sql(\n",
    "            \"\"\"\n",
    "SELECT data_type, COUNT(*) as count\n",
    "FROM bronze_all_data\n",
    "GROUP BY data_type\n",
    "ORDER BY count DESC\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\\nTest 2: States with most data\")\n",
    "    display(\n",
    "        spark.sql(\n",
    "            \"\"\"\n",
    "SELECT state, COUNT(*) as records\n",
    "FROM bronze_all_data\n",
    "WHERE state != 'Unknown'\n",
    "GROUP BY state\n",
    "ORDER BY records DESC\n",
    "LIMIT 5\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\\nTest 3: Latest data sample\")\n",
    "    display(\n",
    "        spark.sql(\n",
    "            \"\"\"\n",
    "SELECT \n",
    "    data_type,\n",
    "    state,\n",
    "    period,\n",
    "    value\n",
    "FROM bronze_all_data\n",
    "WHERE period LIKE '2025-12-20%'\n",
    "ORDER BY period DESC\n",
    "LIMIT 3\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Test queries failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83D\uDE80 READY FOR SILVER LAYER PROCESSING!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nYour bronze layer is now available as temporary views.\")\n",
    "print(\"Proceed to create the Silver Layer for data cleaning and transformation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "384b2419-258c-41cb-a445-b29be28b0190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 4: SILVER LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37a96396-5857-4d96-a3bc-3ad395626d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A Silver Layer realizou a limpeza, padronização e enriquecimento dos dados brutos, convertendo valores para formato numérico, tratando dados ausentes, padronizando nomes de estados e unidades de medida, além de adicionar colunas temporais (timestamp, data, hora) para análise temporal. Esta camada de qualidade garantida unificou 129 DataFrames em uma estrutura consolidada pronta para modelagem analítica, mantendo rastreabilidade através de metadados de processamento enquanto prepara os dados para consumo na camada Gold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5674f50c-2304-4f80-b0f8-1210a029a6b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n\uD83E\uDD48 SILVER LAYER - FIXED FOR ISO FORMAT (YYYY-MM-DDTHH)\n================================================================================\n\n\uD83E\uDDF9 Cleaning up existing silver views...\n  ✅ Dropped: silver_all_data\n  ✅ Dropped: silver_coal\n  ✅ Dropped: silver_demand\n  ✅ Dropped: silver_gas\n  ✅ Dropped: silver_metadata\n  ✅ Dropped: silver_wind\n\n\uD83D\uDD04 Creating silver_all_data with ISO format parsing...\n\n✅ Created silver_all_data\n\n\uD83D\uDCCA Statistics:\n• Total records: 92,793\n• Records with timestamps: 92,793 (100.0%)\n\n\uD83D\uDC41️  Sample data (with parsed timestamps):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>period</th><th>period_ts</th><th>period_date</th><th>period_hour</th><th>state</th><th>data_type</th><th>value_cleaned</th><th>value_units</th></tr></thead><tbody><tr><td>2025-11-21T00</td><td>2025-11-21T00:00:00.000Z</td><td>2025-11-21</td><td>0</td><td>Ohio</td><td>demand</td><td>100148.0</td><td>megawatthours</td></tr><tr><td>2025-11-21T00</td><td>2025-11-21T00:00:00.000Z</td><td>2025-11-21</td><td>0</td><td>Florida</td><td>demand</td><td>6955.0</td><td>megawatthours</td></tr><tr><td>2025-11-21T00</td><td>2025-11-21T00:00:00.000Z</td><td>2025-11-21</td><td>0</td><td>California</td><td>demand</td><td>26814.0</td><td>megawatthours</td></tr><tr><td>2025-11-21T00</td><td>2025-11-21T00:00:00.000Z</td><td>2025-11-21</td><td>0</td><td>California</td><td>demand</td><td>26814.0</td><td>megawatthours</td></tr><tr><td>2025-11-21T00</td><td>2025-11-21T00:00:00.000Z</td><td>2025-11-21</td><td>0</td><td>Florida</td><td>demand</td><td>6955.0</td><td>megawatthours</td></tr><tr><td>2025-11-21T00</td><td>2025-11-21T00:00:00.000Z</td><td>2025-11-21</td><td>0</td><td>Florida</td><td>demand</td><td>6955.0</td><td>megawatthours</td></tr><tr><td>2025-11-21T00</td><td>2025-11-21T00:00:00.000Z</td><td>2025-11-21</td><td>0</td><td>Ohio</td><td>demand</td><td>100148.0</td><td>megawatthours</td></tr><tr><td>2025-11-21T00</td><td>2025-11-21T00:00:00.000Z</td><td>2025-11-21</td><td>0</td><td>Ohio</td><td>demand</td><td>100148.0</td><td>megawatthours</td></tr><tr><td>2025-11-21T00</td><td>2025-11-21T00:00:00.000Z</td><td>2025-11-21</td><td>0</td><td>California</td><td>demand</td><td>26814.0</td><td>megawatthours</td></tr><tr><td>2025-11-21T00</td><td>2025-11-21T00:00:00.000Z</td><td>2025-11-21</td><td>0</td><td>New York</td><td>demand</td><td>19327.0</td><td>megawatthours</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-11-21T00",
         "2025-11-21T00:00:00.000Z",
         "2025-11-21",
         0,
         "Ohio",
         "demand",
         100148.0,
         "megawatthours"
        ],
        [
         "2025-11-21T00",
         "2025-11-21T00:00:00.000Z",
         "2025-11-21",
         0,
         "Florida",
         "demand",
         6955.0,
         "megawatthours"
        ],
        [
         "2025-11-21T00",
         "2025-11-21T00:00:00.000Z",
         "2025-11-21",
         0,
         "California",
         "demand",
         26814.0,
         "megawatthours"
        ],
        [
         "2025-11-21T00",
         "2025-11-21T00:00:00.000Z",
         "2025-11-21",
         0,
         "California",
         "demand",
         26814.0,
         "megawatthours"
        ],
        [
         "2025-11-21T00",
         "2025-11-21T00:00:00.000Z",
         "2025-11-21",
         0,
         "Florida",
         "demand",
         6955.0,
         "megawatthours"
        ],
        [
         "2025-11-21T00",
         "2025-11-21T00:00:00.000Z",
         "2025-11-21",
         0,
         "Florida",
         "demand",
         6955.0,
         "megawatthours"
        ],
        [
         "2025-11-21T00",
         "2025-11-21T00:00:00.000Z",
         "2025-11-21",
         0,
         "Ohio",
         "demand",
         100148.0,
         "megawatthours"
        ],
        [
         "2025-11-21T00",
         "2025-11-21T00:00:00.000Z",
         "2025-11-21",
         0,
         "Ohio",
         "demand",
         100148.0,
         "megawatthours"
        ],
        [
         "2025-11-21T00",
         "2025-11-21T00:00:00.000Z",
         "2025-11-21",
         0,
         "California",
         "demand",
         26814.0,
         "megawatthours"
        ],
        [
         "2025-11-21T00",
         "2025-11-21T00:00:00.000Z",
         "2025-11-21",
         0,
         "New York",
         "demand",
         19327.0,
         "megawatthours"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "period",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "period_ts",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "period_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "period_hour",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value_cleaned",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "value_units",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n\uD83D\uDCC1 CREATING INDIVIDUAL SILVER VIEWS\n================================================================================\n✅ silver_demand        -   25,956 records\n✅ silver_coal          -   23,721 records\n✅ silver_gas           -   25,884 records\n✅ silver_wind          -   17,232 records\n\n================================================================================\n✅ SILVER LAYER COMPLETE!\n================================================================================\n\n✅ Timestamps successfully parsed from ISO format: YYYY-MM-DDTHH\n✅ Added period_hour column for easy hour-based analysis\n✅ Ready for Gold layer star schema creation!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# \uD83E\uDD48 SILVER LAYER - FIXED FOR ISO FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83E\uDD48 SILVER LAYER - FIXED FOR ISO FORMAT (YYYY-MM-DDTHH)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clean up\n",
    "print(\"\\n\uD83E\uDDF9 Cleaning up existing silver views...\")\n",
    "existing_silver_views = [v.name for v in spark.catalog.listTables() \n",
    "                         if v.name.startswith('silver_')]\n",
    "for view in existing_silver_views:\n",
    "    try:\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {view}\")\n",
    "        print(f\"  ✅ Dropped: {view}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Create silver_all_data with the CORRECT format\n",
    "print(\"\\n\uD83D\uDD04 Creating silver_all_data with ISO format parsing...\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW silver_all_data AS\n",
    "    SELECT\n",
    "        -- Parse the ISO format: 2025-11-21T00 → 2025-11-21 00:00:00\n",
    "        CASE \n",
    "            WHEN period LIKE '____-__-__T__' AND LENGTH(period) = 13\n",
    "            THEN to_timestamp(\n",
    "                CONCAT(\n",
    "                    SUBSTRING(period, 1, 10),  -- Date part: 2025-11-21\n",
    "                    ' ',\n",
    "                    SUBSTRING(period, 12, 2),  -- Hour part: 00\n",
    "                    ':00:00'                    -- Add minutes and seconds\n",
    "                ),\n",
    "                'yyyy-MM-dd HH:mm:ss'\n",
    "            )\n",
    "            ELSE NULL\n",
    "        END AS period_ts,\n",
    "        \n",
    "        -- Extract date part\n",
    "        CASE \n",
    "            WHEN period LIKE '____-__-__T__' AND LENGTH(period) = 13\n",
    "            THEN to_date(SUBSTRING(period, 1, 10), 'yyyy-MM-dd')\n",
    "            ELSE NULL\n",
    "        END AS period_date,\n",
    "        \n",
    "        -- Original period for reference\n",
    "        period,\n",
    "        \n",
    "        -- Extract hour from period (e.g., get '00' from '2025-11-21T00')\n",
    "        CASE \n",
    "            WHEN period LIKE '____-__-__T__' AND LENGTH(period) = 13\n",
    "            THEN CAST(SUBSTRING(period, 12, 2) AS INT)\n",
    "            ELSE NULL\n",
    "        END AS period_hour,\n",
    "        \n",
    "        -- Clean the value\n",
    "        CASE \n",
    "            WHEN TRIM(value) = '' THEN NULL\n",
    "            WHEN TRY_CAST(value AS DOUBLE) IS NOT NULL \n",
    "            AND TRY_CAST(value AS DOUBLE) >= 0 \n",
    "            THEN CAST(value AS DOUBLE)\n",
    "            ELSE NULL \n",
    "        END AS value_cleaned,\n",
    "        \n",
    "        -- Original value\n",
    "        value AS value_original,\n",
    "        \n",
    "        -- All other columns\n",
    "        data_type,\n",
    "        state,\n",
    "        ba_code,\n",
    "        COALESCE(respondent_name, 'Unknown') AS respondent_name,\n",
    "        COALESCE(type_name, 'Unknown') AS type_name,\n",
    "        COALESCE(value_units, 'Unknown') AS value_units,\n",
    "        source_file,\n",
    "        ingestion_timestamp,\n",
    "        record_id,\n",
    "        \n",
    "        -- Processing metadata\n",
    "        CURRENT_TIMESTAMP() AS silver_processing_timestamp\n",
    "        \n",
    "    FROM bronze_all_data\n",
    "    WHERE period IS NOT NULL\n",
    "    AND value IS NOT NULL\n",
    "    AND period LIKE '____-__-__T__'  -- Ensure it matches our expected format\n",
    "\"\"\")\n",
    "\n",
    "# Check what we created\n",
    "print(\"\\n✅ Created silver_all_data\")\n",
    "print(\"\\n\uD83D\uDCCA Statistics:\")\n",
    "total_count = spark.sql(\"SELECT COUNT(*) FROM silver_all_data\").collect()[0][0]\n",
    "period_ts_count = spark.sql(\"SELECT COUNT(*) FROM silver_all_data WHERE period_ts IS NOT NULL\").collect()[0][0]\n",
    "\n",
    "print(f\"• Total records: {total_count:,}\")\n",
    "print(f\"• Records with timestamps: {period_ts_count:,} ({period_ts_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n\uD83D\uDC41️  Sample data (with parsed timestamps):\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        period,\n",
    "        period_ts,\n",
    "        period_date,\n",
    "        period_hour,\n",
    "        state,\n",
    "        data_type,\n",
    "        value_cleaned,\n",
    "        value_units\n",
    "    FROM silver_all_data \n",
    "    WHERE period_ts IS NOT NULL\n",
    "    ORDER BY period_ts\n",
    "    LIMIT 10\n",
    "\"\"\"))\n",
    "\n",
    "# Create individual views by data type\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83D\uDCC1 CREATING INDIVIDUAL SILVER VIEWS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for data_type in ['demand', 'coal', 'gas', 'wind']:\n",
    "    silver_view = f\"silver_{data_type}\"\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW {silver_view} AS\n",
    "        SELECT *\n",
    "        FROM silver_all_data\n",
    "        WHERE data_type = '{data_type}'\n",
    "    \"\"\")\n",
    "    count = spark.sql(f\"SELECT COUNT(*) FROM {silver_view}\").collect()[0][0]\n",
    "    print(f\"✅ {silver_view:20} - {count:8,} records\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ SILVER LAYER COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✅ Timestamps successfully parsed from ISO format: YYYY-MM-DDTHH\")\n",
    "print(f\"✅ Added period_hour column for easy hour-based analysis\")\n",
    "print(f\"✅ Ready for Gold layer star schema creation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a21f0674-7d8a-4faf-872d-21f6ee5b60a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 5: GOLD LAYER - STAR SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "340a9e7e-9f5e-4055-a73a-8f0da6b223f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\uD83C\uDFC6 Gold Layer - Camada de Modelagem Analítica\n",
    "A Gold Layer representa a camada final de modelagem dimensional, implementando um esquema estrela otimizado para análise de desempenho e consumo pela camada de visualização (Power BI). Esta camada transforma os dados da Silver em tabelas factuais e dimensionais permanentes, organizadas para consultas analíticas eficientes através do Databricks SQL Serverless.\n",
    "\n",
    "⭐ Esquema Estrela Implementado\n",
    "\uD83C\uDF1F Tabela Fato Principal: gold_fact_energy_measurements\n",
    "Propósito: Registros detalhados de medições horárias\n",
    "\n",
    "Contém: 721 registros horários × 8 estados × 4 tipos de dados\n",
    "\n",
    "Campos-chave:\n",
    "\n",
    "measurement_id: Identificador único (monotonically_increasing_id)\n",
    "\n",
    "measurement_timestamp: Data/hora completa da medição\n",
    "\n",
    "measurement_date e measurement_hour: Componentes temporais\n",
    "\n",
    "state: Estado (California, Texas, Florida, etc.)\n",
    "\n",
    "ba_code: Código da autoridade de balanceamento (CISO, ERCO, PJM, etc.)\n",
    "\n",
    "data_type: Tipo de dado (demand, coal, gas, wind)\n",
    "\n",
    "measurement_value: Valor limpo em MWh\n",
    "\n",
    "original_value: Valor bruto preservado\n",
    "\n",
    "value_units: Unidades de medida (megawatthours)\n",
    "\n",
    "Metadados: source_file, ingestion_timestamp, silver_processing_timestamp\n",
    "\n",
    "\uD83D\uDCCA Tabela Fato Agregada: gold_fact_daily_energy\n",
    "Propósito: Agregações diárias otimizadas para visualizações\n",
    "\n",
    "Contém: Agregações diárias por estado e tipo de energia\n",
    "\n",
    "Métricas:\n",
    "\n",
    "total_value: Soma diária em MWh\n",
    "\n",
    "avg_value: Média horária diária\n",
    "\n",
    "min_value/max_value: Valores extremos do dia\n",
    "\n",
    "measurement_count: Número de medições horárias\n",
    "\n",
    "Dimensões: date, state, data_type\n",
    "\n",
    "\uD83D\uDDD3️ Dimensão de Data: gold_dim_date\n",
    "Contém: Todos os dias presentes nos dados factuais\n",
    "\n",
    "Atributos:\n",
    "\n",
    "date: Data no formato YYYY-MM-DD\n",
    "\n",
    "year, month, day: Componentes numéricos\n",
    "\n",
    "quarter: Trimestre (1-4)\n",
    "\n",
    "month_name, day_name: Nomes por extenso\n",
    "\n",
    "day_of_week, day_of_year: Referências temporais\n",
    "\n",
    "weekday_weekend: Classificação dia útil/fim de semana\n",
    "\n",
    "\uD83D\uDCCD Dimensão de Localização: gold_dim_location\n",
    "Contém: Estados únicos e seus códigos BA\n",
    "\n",
    "Atributos:\n",
    "\n",
    "location_id: Identificador numérico sequencial\n",
    "\n",
    "state: Nome do estado (10 estados incluídos)\n",
    "\n",
    "ba_code: Código da autoridade de balanceamento\n",
    "\n",
    "market_category: Classificação (Major Market/Other Market)\n",
    "\n",
    "Major Market: California, Texas, Florida, New York\n",
    "\n",
    "Other Market: Demais estados\n",
    "\n",
    "⚡ Dimensão de Tipo de Energia: gold_dim_energy_type\n",
    "Contém: Tipos de dados energéticos únicos\n",
    "\n",
    "Atributos:\n",
    "\n",
    "energy_type_id: Identificador numérico sequencial\n",
    "\n",
    "data_type: Código (demand, coal, gas, wind)\n",
    "\n",
    "energy_type_name: Descrição por extenso\n",
    "\n",
    "demand → \"Electricity Demand\"\n",
    "\n",
    "coal → \"Coal Generation\"\n",
    "\n",
    "gas → \"Natural Gas Generation\"\n",
    "\n",
    "wind → \"Wind Generation\"\n",
    "\n",
    "fuel_category: Classificação por categoria de combustível\n",
    "\n",
    "Fossil Fuel: coal, gas\n",
    "\n",
    "Renewable: wind\n",
    "\n",
    "Load: demand\n",
    "\n",
    "Other: demais tipos\n",
    "\n",
    "\uD83C\uDFAF Objetivos do Esquema Estrela\n",
    "Desempenho: Consultas rápidas através de junções simples\n",
    "\n",
    "Simplicidade: Modelo intuitivo para usuários de negócio\n",
    "\n",
    "Flexibilidade: Suporte a múltiplos níveis de granularidade\n",
    "\n",
    "Consistência: Dimensões compartilhadas entre fatos\n",
    "\n",
    "Histórico: Preservação de dados históricos para tendências\n",
    "\n",
    "\uD83D\uDCC8 Casos de Uso Habilitados\n",
    "Análise Temporal:\n",
    "Tendências horárias/diárias de demanda\n",
    "\n",
    "Sazonalidade por dia da semana\n",
    "\n",
    "Padrões \"Duck Curve\" (Califórnia)\n",
    "\n",
    "Análise Geográfica:\n",
    "Comparativo entre estados\n",
    "\n",
    "Distribuição por região\n",
    "\n",
    "Performance por autoridade de balanceamento\n",
    "\n",
    "Análise de Mix Energético:\n",
    "Participação de fontes (carvão, gás, eólica)\n",
    "\n",
    "Transição energética por estado\n",
    "\n",
    "Correlação demanda-geração\n",
    "\n",
    "Business Intelligence:\n",
    "KPIs de desempenho do grid\n",
    "\n",
    "Alertas de picos de demanda\n",
    "\n",
    "Planejamento de capacidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "08269c4b-7d92-407f-ab15-e0c454c49237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\n\uD83E\uDD47 GOLD LAYER - PERMANENT TABLES\n================================================================================\nCreating permanent tables for Serverless SQL access...\n\n\uD83D\uDCCB Current catalog/database:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>current_catalog()</th><th>current_database()</th></tr></thead><tbody><tr><td>workspace</td><td>default</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "workspace",
         "default"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "current_catalog()",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "current_database()",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\uD83E\uDDF9 Cleaning up existing tables...\n✅ Cleanup complete\n\n1. Creating permanent fact table...\n✅ Created: gold_fact_energy_measurements (91,299 records)\n\n2. Creating daily aggregated table for area chart...\n✅ Created: gold_fact_daily_energy (911 records)\n\n3. Creating dimension tables...\n✅ Created: gold_dim_date\n✅ Created: gold_dim_location\n✅ Created: gold_dim_energy_type\n\n================================================================================\n✅ PERMANENT GOLD LAYER TABLES CREATED!\n================================================================================\n\n\uD83C\uDF89 Permanent tables created! Access them from Serverless SQL with:\n\n1. Your area chart query:\n   SELECT \n     date,\n     state,\n     SUM(total_value) AS total_generation_mw\n   FROM gold_fact_daily_energy\n   WHERE data_type IN ('coal', 'gas', 'wind')\n     AND state IS NOT NULL\n     AND state != 'Unknown'\n   GROUP BY date, state\n   ORDER BY date, state;\n\n2. Test query:\n   SELECT * FROM gold_fact_daily_energy \n   WHERE data_type IN ('coal', 'gas', 'wind')\n   LIMIT 5;\n\n\uD83D\uDCCB Available permanent tables:\n   • gold_fact_energy_measurements\n   • gold_fact_daily_energy\n   • gold_dim_date\n   • gold_dim_location\n   • gold_dim_energy_type\n\n\n\uD83E\uDDEA Quick test - sample from daily aggregated:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>date</th><th>state</th><th>data_type</th><th>total_value</th><th>avg_value</th><th>min_value</th><th>max_value</th><th>measurement_count</th></tr></thead><tbody><tr><td>2025-12-17</td><td>Illinois</td><td>gas</td><td>1804449.0</td><td>25061.791666666668</td><td>21766.0</td><td>31778.0</td><td>72</td></tr><tr><td>2025-12-08</td><td>Illinois</td><td>gas</td><td>2261835.0</td><td>31414.375</td><td>26637.0</td><td>36941.0</td><td>72</td></tr><tr><td>2025-12-08</td><td>Georgia</td><td>gas</td><td>6182038.0</td><td>17172.327777777777</td><td>2888.0</td><td>57462.0</td><td>360</td></tr><tr><td>2025-12-10</td><td>California</td><td>wind</td><td>120198.0</td><td>834.7083333333334</td><td>428.0</td><td>1351.0</td><td>144</td></tr><tr><td>2025-12-17</td><td>California</td><td>wind</td><td>312309.0</td><td>2168.8125</td><td>479.0</td><td>3313.0</td><td>144</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-12-17",
         "Illinois",
         "gas",
         1804449.0,
         25061.791666666668,
         21766.0,
         31778.0,
         72
        ],
        [
         "2025-12-08",
         "Illinois",
         "gas",
         2261835.0,
         31414.375,
         26637.0,
         36941.0,
         72
        ],
        [
         "2025-12-08",
         "Georgia",
         "gas",
         6182038.0,
         17172.327777777777,
         2888.0,
         57462.0,
         360
        ],
        [
         "2025-12-10",
         "California",
         "wind",
         120198.0,
         834.7083333333334,
         428.0,
         1351.0,
         144
        ],
        [
         "2025-12-17",
         "California",
         "wind",
         312309.0,
         2168.8125,
         479.0,
         3313.0,
         144
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_value",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_value",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "min_value",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "max_value",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "measurement_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# \uD83E\uDD47 GOLD LAYER - PERMANENT TABLES FOR SERVERLESS SQL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\uD83E\uDD47 GOLD LAYER - PERMANENT TABLES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Creating permanent tables for Serverless SQL access...\")\n",
    "\n",
    "# First, check what catalog/database we're in\n",
    "print(\"\\n\uD83D\uDCCB Current catalog/database:\")\n",
    "display(spark.sql(\"SELECT current_catalog(), current_database()\"))\n",
    "\n",
    "# ============================================================================\n",
    "# CLEANUP EXISTING TABLES\n",
    "# ============================================================================\n",
    "print(\"\\n\uD83E\uDDF9 Cleaning up existing tables...\")\n",
    "try:\n",
    "    spark.sql(\"DROP TABLE IF EXISTS gold_fact_energy_measurements\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS gold_fact_daily_energy\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS gold_dim_date\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS gold_dim_location\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS gold_dim_energy_type\")\n",
    "    print(\"✅ Cleanup complete\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Cleanup warning: {str(e)[:50]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. MAIN FACT TABLE (PERMANENT TABLE)\n",
    "# ============================================================================\n",
    "print(\"\\n1. Creating permanent fact table...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE gold_fact_energy_measurements AS\n",
    "    SELECT\n",
    "        -- Generate a unique ID\n",
    "        monotonically_increasing_id() AS measurement_id,\n",
    "        \n",
    "        -- Time dimensions\n",
    "        period_ts AS measurement_timestamp,\n",
    "        period_date AS measurement_date,\n",
    "        period_hour AS measurement_hour,\n",
    "        \n",
    "        -- Location\n",
    "        state,\n",
    "        ba_code,\n",
    "        \n",
    "        -- Energy type\n",
    "        data_type,\n",
    "        \n",
    "        -- Values\n",
    "        value_cleaned AS measurement_value,\n",
    "        value_original AS original_value,\n",
    "        value_units,\n",
    "        \n",
    "        -- Additional info\n",
    "        respondent_name,\n",
    "        type_name,\n",
    "        source_file,\n",
    "        \n",
    "        -- Metadata\n",
    "        ingestion_timestamp,\n",
    "        silver_processing_timestamp\n",
    "        \n",
    "    FROM silver_all_data\n",
    "    WHERE value_cleaned IS NOT NULL\n",
    "      AND period_ts IS NOT NULL\n",
    "      AND period_date IS NOT NULL\n",
    "\"\"\")\n",
    "fact_count = spark.sql(\"SELECT COUNT(*) FROM gold_fact_energy_measurements\").collect()[0][0]\n",
    "print(f\"✅ Created: gold_fact_energy_measurements ({fact_count:,} records)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DAILY AGGREGATED TABLE (FOR YOUR AREA CHART)\n",
    "# ============================================================================\n",
    "print(\"\\n2. Creating daily aggregated table for area chart...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE gold_fact_daily_energy AS\n",
    "    SELECT\n",
    "        measurement_date AS date,\n",
    "        state,\n",
    "        data_type,\n",
    "        SUM(measurement_value) AS total_value,\n",
    "        AVG(measurement_value) AS avg_value,\n",
    "        MIN(measurement_value) AS min_value,\n",
    "        MAX(measurement_value) AS max_value,\n",
    "        COUNT(*) AS measurement_count\n",
    "    FROM gold_fact_energy_measurements\n",
    "    WHERE measurement_value IS NOT NULL\n",
    "      AND measurement_date IS NOT NULL\n",
    "    GROUP BY measurement_date, state, data_type\n",
    "\"\"\")\n",
    "daily_count = spark.sql(\"SELECT COUNT(*) FROM gold_fact_daily_energy\").collect()[0][0]\n",
    "print(f\"✅ Created: gold_fact_daily_energy ({daily_count:,} records)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DIMENSION TABLES (PERMANENT TABLES)\n",
    "# ============================================================================\n",
    "print(\"\\n3. Creating dimension tables...\")\n",
    "\n",
    "# Dim Date\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE gold_dim_date AS\n",
    "    WITH date_range AS (\n",
    "        SELECT DISTINCT measurement_date AS date\n",
    "        FROM gold_fact_energy_measurements\n",
    "        WHERE measurement_date IS NOT NULL\n",
    "    )\n",
    "    SELECT\n",
    "        date,\n",
    "        YEAR(date) AS year,\n",
    "        MONTH(date) AS month,\n",
    "        DAY(date) AS day,\n",
    "        DAYOFWEEK(date) AS day_of_week,\n",
    "        DAYOFYEAR(date) AS day_of_year,\n",
    "        QUARTER(date) AS quarter,\n",
    "        DATE_FORMAT(date, 'MMMM') AS month_name,\n",
    "        DATE_FORMAT(date, 'EEEE') AS day_name,\n",
    "        CASE \n",
    "            WHEN DAYOFWEEK(date) IN (1, 7) THEN 'Weekend'\n",
    "            ELSE 'Weekday'\n",
    "        END AS weekday_weekend\n",
    "    FROM date_range\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "print(\"✅ Created: gold_dim_date\")\n",
    "\n",
    "# Dim Location\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE gold_dim_location AS\n",
    "    SELECT DISTINCT\n",
    "        ROW_NUMBER() OVER (ORDER BY state) AS location_id,\n",
    "        state,\n",
    "        ba_code,\n",
    "        CASE \n",
    "            WHEN state IN ('California', 'Texas', 'Florida', 'New York') \n",
    "            THEN 'Major Market'\n",
    "            ELSE 'Other Market'\n",
    "        END AS market_category\n",
    "    FROM gold_fact_energy_measurements\n",
    "    WHERE state IS NOT NULL AND state != 'Unknown'\n",
    "    ORDER BY state\n",
    "\"\"\")\n",
    "print(\"✅ Created: gold_dim_location\")\n",
    "\n",
    "# Dim Energy Type\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE gold_dim_energy_type AS\n",
    "    SELECT DISTINCT\n",
    "        ROW_NUMBER() OVER (ORDER BY data_type) AS energy_type_id,\n",
    "        data_type,\n",
    "        CASE \n",
    "            WHEN data_type = 'demand' THEN 'Electricity Demand'\n",
    "            WHEN data_type = 'coal' THEN 'Coal Generation'\n",
    "            WHEN data_type = 'gas' THEN 'Natural Gas Generation'\n",
    "            WHEN data_type = 'wind' THEN 'Wind Generation'\n",
    "            ELSE data_type\n",
    "        END AS energy_type_name,\n",
    "        CASE \n",
    "            WHEN data_type IN ('coal', 'gas') THEN 'Fossil Fuel'\n",
    "            WHEN data_type = 'wind' THEN 'Renewable'\n",
    "            WHEN data_type = 'demand' THEN 'Load'\n",
    "            ELSE 'Other'\n",
    "        END AS fuel_category\n",
    "    FROM gold_fact_energy_measurements\n",
    "    WHERE data_type IS NOT NULL\n",
    "    ORDER BY data_type\n",
    "\"\"\")\n",
    "print(\"✅ Created: gold_dim_energy_type\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ PERMANENT GOLD LAYER TABLES CREATED!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "\uD83C\uDF89 Permanent tables created! Access them from Serverless SQL with:\n",
    "\n",
    "1. Your area chart query:\n",
    "   SELECT \n",
    "     date,\n",
    "     state,\n",
    "     SUM(total_value) AS total_generation_mw\n",
    "   FROM gold_fact_daily_energy\n",
    "   WHERE data_type IN ('coal', 'gas', 'wind')\n",
    "     AND state IS NOT NULL\n",
    "     AND state != 'Unknown'\n",
    "   GROUP BY date, state\n",
    "   ORDER BY date, state;\n",
    "\n",
    "2. Test query:\n",
    "   SELECT * FROM gold_fact_daily_energy \n",
    "   WHERE data_type IN ('coal', 'gas', 'wind')\n",
    "   LIMIT 5;\n",
    "\n",
    "\uD83D\uDCCB Available permanent tables:\n",
    "   • gold_fact_energy_measurements\n",
    "   • gold_fact_daily_energy\n",
    "   • gold_dim_date\n",
    "   • gold_dim_location\n",
    "   • gold_dim_energy_type\n",
    "\"\"\")\n",
    "\n",
    "# Quick test\n",
    "print(\"\\n\uD83E\uDDEA Quick test - sample from daily aggregated:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT * FROM gold_fact_daily_energy \n",
    "    WHERE data_type IN ('coal', 'gas', 'wind')\n",
    "    LIMIT 5\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MVP  STAR SCHEMA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}