{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56a0dfbd-af67-4e21-8062-ad6014039711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "MVP  STAR SCHEMA\n",
    "MVP Engenharia de dados Matr√≠cula:4052025000027 \n",
    "Dataset: IEA Energy Demand and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09432b88-5330-4c6d-9a68-94114ea186e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A proposta deste trabalho √© importar dados dos 10 estados que mais consomem energia el√©trica nos estados unidos e analisar o perfil de gera√ß√£o el√©trica de cada estado, olhando para as 3 principais fontes: G√°s Natural, Carv√£o e E√≥lica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef7dd7c3-b681-451b-89f0-b3e993d57904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 1: IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde7f113-036d-4357-9c2c-ddcdbe2c0831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "print(\"\\nüìö STEP 1: Importing libraries...\")\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "from pyspark.sql.functions import lit, current_timestamp, col, when\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ea75ba-815d-40ba-986a-1bf7a7ad69af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 2: API CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79a496b3-17cf-48d9-9da2-7bc41577bb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the full workspace path for your notebook\n",
    "dbutils.notebook.run(\n",
    "    \"/Workspace/Users/lucasrobertideoliveira@hotmail.com/SETUP SECRETS\",\n",
    "    60\n",
    ")\n",
    "#  ============================================================================\n",
    "# üìä FETCH ELECTRICITY DATA FOR 10 US STATES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä FETCHING ELECTRICITY DATA FOR 10 US STATES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CONFIGURE DATE RANGE\n",
    "# ============================================================================\n",
    "print(\"\\nüìÖ Configuring date range...\")\n",
    "end_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "start_date = end_date - timedelta(days=30)  # 30 days for testing\n",
    "\n",
    "print(f\"   Start Date: {start_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"   End Date: {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"   Total Days: {(end_date - start_date).days}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. SETUP EIA API CONFIGURATION (DATABRICKS SECRETS VERSION)\n",
    "# ============================================================================\n",
    "print(\"\\nüîß Setting up EIA API...\")\n",
    "\n",
    "def load_eia_api_key():\n",
    "    try:\n",
    "        api_key = dbutils.secrets.get(\n",
    "            scope=\"eia-scope\",\n",
    "            key=\"eia_api_key\"\n",
    "        )\n",
    "        if api_key and api_key.strip():\n",
    "            masked = api_key[:5] + \"...\" + api_key[-5:] if len(api_key) > 10 else \"***\"\n",
    "            print(f\"   ‚úÖ Loaded from Databricks Secrets: {masked}\")\n",
    "            return api_key.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    import os\n",
    "    api_key = os.environ.get(\"EIA_API_KEY\")\n",
    "    if api_key and api_key.strip():\n",
    "        masked = api_key[:5] + \"...\" + api_key[-5:] if len(api_key) > 10 else \"***\"\n",
    "        print(f\"   ‚úÖ Loaded from environment variable: {masked}\")\n",
    "        return api_key.strip()\n",
    "\n",
    "    # Check if variable is defined from SETUP_SECRETS\n",
    "    try:\n",
    "        api_key = YOUR_REAL_EIA_API_KEY\n",
    "        if api_key and api_key.strip():\n",
    "            masked = api_key[:5] + \"...\" + api_key[-5:] if len(api_key) > 10 else \"***\"\n",
    "            print(f\"   ‚úÖ Loaded from variable: {masked}\")\n",
    "            return api_key.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"\\n   ‚ùå ERROR: No API key found in any source!\")\n",
    "    raise ValueError(\"EIA API Key is required. Run SETUP_SECRETS notebook first.\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TARGET STATES AND THEIR BA CODES\n",
    "# ============================================================================\n",
    "print(\"\\nüéØ Target States and Their Balancing Authority Codes:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Based on your working codes and EIA documentation\n",
    "state_ba_mapping = {\n",
    "    # California - Using CAL (California) - this works\n",
    "    \"California\": [\"CAL\", \"CISO\"],\n",
    "    \n",
    "    # Texas - Using ERCO (Electric Reliability Council of Texas) - this works\n",
    "    \"Texas\": [\"ERCO\"],\n",
    "    \n",
    "    # Florida - Using FPL (Florida Power & Light) - this works\n",
    "    \"Florida\": [\"FPL\", \"TECO\", \"FPC\"],\n",
    "    \n",
    "    # Ohio - Part of PJM, using FE (FirstEnergy) - this works\n",
    "    \"Ohio\": [\"FE\", \"PJM\"],\n",
    "    \n",
    "    # Georgia - Using SOCO (Southern Company) - this works\n",
    "    \"Georgia\": [\"SOCO\"],\n",
    "    \n",
    "    # New York - Using NY (New York) and NYIS (New York ISO)\n",
    "    \"New York\": [\"NY\", \"NYIS\"],\n",
    "    \n",
    "    # Pennsylvania - Part of PJM, using PJM\n",
    "    \"Pennsylvania\": [\"PJM\", \"FE\"],\n",
    "    \n",
    "    # North Carolina - Using DUK (Duke Energy) and CAR (Carolinas)\n",
    "    \"North Carolina\": [\"DUK\", \"CAR\"],\n",
    "    \n",
    "    # Virginia - Part of PJM, using PJM\n",
    "    \"Virginia\": [\"PJM\"],\n",
    "    \n",
    "    # Illinois - Using MISO (Midcontinent ISO)\n",
    "    \"Illinois\": [\"MISO\"]\n",
    "}\n",
    "\n",
    "# Display mapping\n",
    "for state, bas in state_ba_mapping.items():\n",
    "    print(f\"  ‚Ä¢ {state:20} ‚Üí {', '.join(bas)}\")\n",
    "\n",
    "# Get all unique BA codes\n",
    "all_ba_codes = []\n",
    "for bas in state_ba_mapping.values():\n",
    "    all_ba_codes.extend(bas)\n",
    "all_ba_codes = list(set(all_ba_codes))\n",
    "\n",
    "print(f\"\\nüìä Total unique BA codes to try: {len(all_ba_codes)}\")\n",
    "print(f\"   Codes: {', '.join(sorted(all_ba_codes))}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CHECK AVAILABLE RESPONDENT CODES\n",
    "# ============================================================================\n",
    "print(\"\\nüîç Checking available respondent codes from EIA API...\")\n",
    "respondent_facet_url = f\"{EIA_BASE_URL}/electricity/rto/region-data/facet/respondent\"\n",
    "respondent_params = {\"api_key\": EIA_API_KEY}\n",
    "\n",
    "available_respondents = {}\n",
    "try:\n",
    "    print(f\"   Getting respondent facet values...\")\n",
    "    respondent_response = requests.get(respondent_facet_url, params=respondent_params, timeout=10)\n",
    "    \n",
    "    if respondent_response.status_code == 200:\n",
    "        respondent_data = respondent_response.json()\n",
    "        \n",
    "        if 'response' in respondent_data and 'facets' in respondent_data['response']:\n",
    "            facets = respondent_data['response']['facets']\n",
    "            print(f\"   ‚úÖ Found {len(facets)} respondent codes total\")\n",
    "            \n",
    "            # Map all available respondents\n",
    "            for facet in facets:\n",
    "                available_respondents[facet['id']] = facet.get('name', 'No name')\n",
    "            \n",
    "            # Check which of our target codes are available\n",
    "            print(\"\\n   Checking availability of our target BA codes:\")\n",
    "            available_codes = []\n",
    "            unavailable_codes = []\n",
    "            \n",
    "            for ba_code in all_ba_codes:\n",
    "                if ba_code in available_respondents:\n",
    "                    available_codes.append(ba_code)\n",
    "                    print(f\"      ‚úÖ {ba_code}: {available_respondents[ba_code]}\")\n",
    "                else:\n",
    "                    unavailable_codes.append(ba_code)\n",
    "                    print(f\"      ‚ùå {ba_code}: Not found in API\")\n",
    "            \n",
    "            print(f\"\\n   üìã Availability Summary:\")\n",
    "            print(f\"      ‚Ä¢ Available: {len(available_codes)}/{len(all_ba_codes)}\")\n",
    "            print(f\"      ‚Ä¢ Unavailable: {len(unavailable_codes)}\")\n",
    "            \n",
    "            # Use only available codes\n",
    "            selected_respondents = available_codes\n",
    "            \n",
    "            # If we don't have enough, add some fallbacks from known working codes\n",
    "            if len(selected_respondents) < 5:\n",
    "                print(\"\\n   ‚ö†Ô∏è Few codes available. Adding known working codes...\")\n",
    "                fallback_codes = [\"CAL\", \"ERCO\", \"SOCO\", \"FE\", \"FPL\", \"CISO\", \"NY\", \"PJM\", \"DUK\", \"MISO\"]\n",
    "                for code in fallback_codes:\n",
    "                    if code not in selected_respondents and code in available_respondents:\n",
    "                        selected_respondents.append(code)\n",
    "                        print(f\"      ‚ûï Added: {code}\")\n",
    "            \n",
    "            print(f\"\\n   ‚úÖ Using respondent codes: {selected_respondents}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è Could not parse respondent data\")\n",
    "            # Use known working codes as fallback\n",
    "            selected_respondents = [\"CAL\", \"ERCO\", \"SOCO\", \"FE\", \"FPL\", \"CISO\", \"NY\", \"PJM\", \"DUK\", \"MISO\"]\n",
    "            print(f\"   ‚ö†Ô∏è Using fallback codes: {selected_respondents}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"   ‚ùå Respondent facet failed: {respondent_response.status_code}\")\n",
    "        # Use known working codes as fallback\n",
    "        selected_respondents = [\"CAL\", \"ERCO\", \"SOCO\", \"FE\", \"FPL\", \"CISO\", \"NY\", \"PJM\", \"DUK\", \"MISO\"]\n",
    "        print(f\"   ‚ö†Ô∏è Using fallback codes: {selected_respondents}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error getting respondent codes: {e}\")\n",
    "    # Use known working codes as fallback\n",
    "    selected_respondents = [\"CAL\", \"ERCO\", \"SOCO\", \"FE\", \"FPL\", \"CISO\", \"NY\", \"PJM\", \"DUK\", \"MISO\"]\n",
    "    print(f\"   ‚ö†Ô∏è Using fallback codes: {selected_respondents}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. FETCH DATA FUNCTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üì• FETCHING ELECTRICITY DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "base_params = {\n",
    "    \"api_key\": EIA_API_KEY,\n",
    "    \"frequency\": \"hourly\",\n",
    "    \"data[]\": \"value\",\n",
    "    \"start\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "    \"end\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "    \"sort[0][column]\": \"period\",\n",
    "    \"sort[0][direction]\": \"asc\",\n",
    "    \"offset\": 0,\n",
    "    \"length\": 5000\n",
    "}\n",
    "\n",
    "datasets_config = {\n",
    "    \"electricity_demand\": {\n",
    "        \"endpoint\": \"electricity/rto/region-data/data\",\n",
    "        \"params\": base_params.copy(),\n",
    "        \"facets[type][]\": \"D\"\n",
    "    },\n",
    "    \"coal_generation\": {\n",
    "        \"endpoint\": \"electricity/rto/fuel-type-data/data\",\n",
    "        \"params\": base_params.copy(),\n",
    "        \"facets[fueltype][]\": \"COL\"\n",
    "    },\n",
    "    \"natural_gas_generation\": {\n",
    "        \"endpoint\": \"electricity/rto/fuel-type-data/data\",\n",
    "        \"params\": base_params.copy(),\n",
    "        \"facets[fueltype][]\": \"NG\"\n",
    "    },\n",
    "    \"wind_generation\": {\n",
    "        \"endpoint\": \"electricity/rto/fuel-type-data/data\",\n",
    "        \"params\": base_params.copy(),\n",
    "        \"facets[fueltype][]\": \"WND\"\n",
    "    }\n",
    "}\n",
    "\n",
    "fetched_data = {}\n",
    "summary = []\n",
    "state_data = {}  # To organize data by state\n",
    "\n",
    "print(\"\\nüìã Fetching datasets for all available BA codes:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, config in datasets_config.items():\n",
    "    data_type = name.replace('_', ' ').title()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä {data_type}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    for respondent in selected_respondents:\n",
    "        print(f\"\\nüìç BA Code: {respondent}\")\n",
    "        \n",
    "        # Find which state this BA belongs to\n",
    "        state_name = None\n",
    "        for state, bas in state_ba_mapping.items():\n",
    "            if respondent in bas:\n",
    "                state_name = state\n",
    "                break\n",
    "        \n",
    "        if state_name:\n",
    "            print(f\"   State: {state_name}\")\n",
    "        \n",
    "        params = config[\"params\"].copy()\n",
    "        params[\"facets[respondent][]\"] = respondent\n",
    "        for key, value in config.items():\n",
    "            if key.startswith(\"facets[\") and key != \"facets[respondent][]\":\n",
    "                params[key] = value\n",
    "        \n",
    "        endpoint = config[\"endpoint\"]\n",
    "        url = f\"{EIA_BASE_URL}/{endpoint}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                records = data.get('response', {}).get('data', [])\n",
    "                \n",
    "                if records and len(records) > 0:\n",
    "                    print(f\"   ‚úÖ SUCCESS: {len(records)} records\")\n",
    "                    \n",
    "                    # Create keys for storage\n",
    "                    raw_key = f\"{name}_{respondent}\"\n",
    "                    state_key = f\"{name}_{state_name.replace(' ', '_')}_{respondent}\" if state_name else raw_key\n",
    "                    \n",
    "                    # Store the data\n",
    "                    fetched_data[raw_key] = records\n",
    "                    fetched_data[state_key] = records  # Also store with state name\n",
    "                    \n",
    "                    # Organize by state\n",
    "                    if state_name:\n",
    "                        if state_name not in state_data:\n",
    "                            state_data[state_name] = {}\n",
    "                        if name not in state_data[state_name]:\n",
    "                            state_data[state_name][name] = []\n",
    "                        state_data[state_name][name].append({\n",
    "                            'ba_code': respondent,\n",
    "                            'records': records,\n",
    "                            'count': len(records)\n",
    "                        })\n",
    "                    \n",
    "                    # Show sample stats\n",
    "                    df = pd.DataFrame(records[:5])  # Just first 5 for stats\n",
    "                    if 'value' in df.columns:\n",
    "                        values = pd.to_numeric(df['value'], errors='coerce')\n",
    "                        valid_values = values.dropna()\n",
    "                        if len(valid_values) > 0:\n",
    "                            print(f\"   üìà Sample values: {valid_values.min():.0f} to {valid_values.max():.0f} MWh\")\n",
    "                    \n",
    "                    summary.append({\n",
    "                        'dataset': state_key if state_name else raw_key,\n",
    "                        'state': state_name if state_name else 'Unknown',\n",
    "                        'ba_code': respondent,\n",
    "                        'data_type': name,\n",
    "                        'records': len(records),\n",
    "                        'status': 'SUCCESS'\n",
    "                    })\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è No records returned\")\n",
    "                    summary.append({\n",
    "                        'dataset': f\"{name}_{respondent}\",\n",
    "                        'state': state_name if state_name else 'Unknown',\n",
    "                        'ba_code': respondent,\n",
    "                        'data_type': name,\n",
    "                        'records': 0,\n",
    "                        'status': 'NO_DATA'\n",
    "                    })\n",
    "                    \n",
    "            else:\n",
    "                print(f\"   ‚ùå HTTP Error {response.status_code}\")\n",
    "                summary.append({\n",
    "                    'dataset': f\"{name}_{respondent}\",\n",
    "                    'state': state_name if state_name else 'Unknown',\n",
    "                    'ba_code': respondent,\n",
    "                    'data_type': name,\n",
    "                    'records': 0,\n",
    "                    'status': f'HTTP_{response.status_code}'\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {str(e)[:100]}\")\n",
    "            summary.append({\n",
    "                'dataset': f\"{name}_{respondent}\",\n",
    "                'state': state_name if state_name else 'Unknown',\n",
    "                'ba_code': respondent,\n",
    "                'data_type': name,\n",
    "                'records': 0,\n",
    "                'status': 'ERROR'\n",
    "            })\n",
    "\n",
    "# ============================================================================\n",
    "# 6. CONVERT TO DATAFRAMES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîÑ CONVERTING TO DATAFRAMES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "if fetched_data:\n",
    "    print(f\"\\nüì¶ Converting {len(fetched_data)} datasets to DataFrames...\")\n",
    "    \n",
    "    conversion_stats = {'success': 0, 'failed': 0}\n",
    "    \n",
    "    for key, records in fetched_data.items():\n",
    "        try:\n",
    "            if records and len(records) > 0:\n",
    "                df = pd.DataFrame(records)\n",
    "                dataframes[key] = df\n",
    "                conversion_stats['success'] += 1\n",
    "                \n",
    "                # Create simplified aliases for easier access\n",
    "                parts = key.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    # Format: type_state_ba or type_ba\n",
    "                    if any(state.replace(' ', '_') in key for state in state_ba_mapping.keys()):\n",
    "                        # Already has state name\n",
    "                        pass\n",
    "                    else:\n",
    "                        # Try to add state name\n",
    "                        ba_code = parts[-1]\n",
    "                        for state, bas in state_ba_mapping.items():\n",
    "                            if ba_code in bas:\n",
    "                                state_alias = f\"{parts[0]}_{state.replace(' ', '_')}_{ba_code}\"\n",
    "                                if state_alias not in dataframes:\n",
    "                                    dataframes[state_alias] = df\n",
    "                                break\n",
    "            else:\n",
    "                conversion_stats['failed'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            conversion_stats['failed'] += 1\n",
    "            print(f\"   ‚ùå Error converting {key}: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Conversion complete:\")\n",
    "    print(f\"   ‚Ä¢ Successful: {conversion_stats['success']}\")\n",
    "    print(f\"   ‚Ä¢ Failed: {conversion_stats['failed']}\")\n",
    "    print(f\"   ‚Ä¢ Total DataFrames: {len(dataframes)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data to convert\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. DISPLAY RESULTS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä FETCHING RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if summary:\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    \n",
    "    # Summary by state\n",
    "    print(\"\\nüó∫Ô∏è DATA BY STATE:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    state_stats = summary_df[summary_df['status'] == 'SUCCESS'].groupby('state').agg({\n",
    "        'data_type': 'nunique',\n",
    "        'records': 'sum',\n",
    "        'ba_code': lambda x: ', '.join(sorted(set(x)))\n",
    "    }).reset_index()\n",
    "    \n",
    "    state_stats.columns = ['State', 'Data Types', 'Total Records', 'BA Codes']\n",
    "    print(state_stats.to_string(index=False))\n",
    "    \n",
    "    # Summary by data type\n",
    "    print(\"\\n‚ö° DATA BY TYPE:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    type_stats = summary_df[summary_df['status'] == 'SUCCESS'].groupby('data_type').agg({\n",
    "        'state': 'nunique',\n",
    "        'records': 'sum',\n",
    "        'dataset': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    type_stats.columns = ['Data Type', 'States', 'Total Records', 'Datasets']\n",
    "    print(type_stats.to_string(index=False))\n",
    "    \n",
    "    # Overall totals\n",
    "    total_success = len(summary_df[summary_df['status'] == 'SUCCESS'])\n",
    "    total_records = summary_df[summary_df['status'] == 'SUCCESS']['records'].sum()\n",
    "    \n",
    "    print(f\"\\nüìà OVERALL TOTALS:\")\n",
    "    print(f\"   ‚Ä¢ Total fetch attempts: {len(summary_df)}\")\n",
    "    print(f\"   ‚Ä¢ Successful fetches: {total_success}\")\n",
    "    print(f\"   ‚Ä¢ Total records: {total_records:,}\")\n",
    "    print(f\"   ‚Ä¢ States with data: {state_stats.shape[0]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå No summary data available\")\n",
    "\n",
    "# ============================================================================\n",
    "# 8. SAMPLE DATA PREVIEW\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üëÅÔ∏è SAMPLE DATA PREVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if dataframes:\n",
    "    # Show samples from different states\n",
    "    sample_states = ['California', 'Texas', 'Florida', 'New York', 'Illinois']\n",
    "    \n",
    "    for state in sample_states:\n",
    "        state_key = state.replace(' ', '_')\n",
    "        state_dfs = [k for k in dataframes.keys() if state_key in k]\n",
    "        \n",
    "        if state_dfs:\n",
    "            print(f\"\\nüèõÔ∏è {state.upper()}:\")\n",
    "            print(f\"   Available datasets: {len(state_dfs)}\")\n",
    "            \n",
    "            # Show first dataset as sample\n",
    "            sample_key = state_dfs[0]\n",
    "            df = dataframes[sample_key]\n",
    "            \n",
    "            print(f\"\\n   üìä {sample_key}:\")\n",
    "            print(f\"      Shape: {df.shape}\")\n",
    "            \n",
    "            if 'period' in df.columns and len(df) > 0:\n",
    "                print(f\"      Time range: {df['period'].iloc[0]} to {df['period'].iloc[-1]}\")\n",
    "            \n",
    "            if 'value' in df.columns and len(df) > 0:\n",
    "                values = pd.to_numeric(df['value'], errors='coerce')\n",
    "                valid_values = values.dropna()\n",
    "                if len(valid_values) > 0:\n",
    "                    print(f\"      Value range: {valid_values.min():.0f} to {valid_values.max():.0f} MWh\")\n",
    "                    print(f\"      Average: {valid_values.mean():.0f} MWh\")\n",
    "            \n",
    "            # Show first 2 rows\n",
    "            print(f\"\\n      First 2 rows:\")\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.width', 1000)\n",
    "            print(df.head(2).to_string())\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\nüèõÔ∏è {state.upper()}: No data available\")\n",
    "    \n",
    "    # List all available DataFrames\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìã ALL AVAILABLE DATAFRAMES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nTotal DataFrames: {len(dataframes)}\")\n",
    "    print(\"\\nFirst 20 DataFrames:\")\n",
    "    for i, key in enumerate(sorted(list(dataframes.keys()))[:20], 1):\n",
    "        df = dataframes[key]\n",
    "        print(f\"{i:3}. {key:50} | {df.shape[0]:6} records\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå No DataFrames available\")\n",
    "\n",
    "# ============================================================================\n",
    "# 9. HOW TO ACCESS YOUR DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üöÄ HOW TO ACCESS YOUR DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "ACCESS EXAMPLES:\n",
    "----------------\n",
    "\n",
    "1. By State and Data Type (Recommended):\n",
    "   -------------------------------------\n",
    "   # Texas electricity demand\n",
    "   df = dataframes['electricity_demand_Texas_ERCO']\n",
    "   \n",
    "   # California natural gas generation  \n",
    "   df = dataframes['natural_gas_generation_California_CAL']\n",
    "   \n",
    "   # Florida wind generation\n",
    "   df = dataframes['wind_generation_Florida_FPL']\n",
    "   \n",
    "   # Illinois coal generation\n",
    "   df = dataframes['coal_generation_Illinois_MISO']\n",
    "\n",
    "2. By BA Code Only:\n",
    "   -----------------\n",
    "   df = dataframes['electricity_demand_ERCO']\n",
    "   df = dataframes['coal_generation_PJM']\n",
    "   df = dataframes['wind_generation_MISO']\n",
    "\n",
    "3. Check All Available:\n",
    "   ---------------------\n",
    "   print(\"Available DataFrames:\", len(dataframes))\n",
    "   print(\"Keys starting with 'Texas':\")\n",
    "   texas_keys = [k for k in dataframes.keys() if 'Texas' in k]\n",
    "   for key in texas_keys:\n",
    "       print(f\"  ‚Ä¢ {key}\")\n",
    "\n",
    "QUICK ANALYSIS EXAMPLES:\n",
    "------------------------\n",
    "# 1. Get Texas demand statistics\n",
    "if 'electricity_demand_Texas_ERCO' in dataframes:\n",
    "    df = dataframes['electricity_demand_Texas_ERCO']\n",
    "    df['value_numeric'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    print(\"Texas Demand Stats:\")\n",
    "    print(f\"  Min: {df['value_numeric'].min():.0f} MWh\")\n",
    "    print(f\"  Max: {df['value_numeric'].max():.0f} MWh\") \n",
    "    print(f\"  Mean: {df['value_numeric'].mean():.0f} MWh\")\n",
    "\n",
    "# 2. Compare states\n",
    "states = ['California', 'Texas', 'Florida']\n",
    "for state in states:\n",
    "    key = f'electricity_demand_{state.replace(\" \", \"_\")}'\n",
    "    matching_keys = [k for k in dataframes.keys() if key in k]\n",
    "    if matching_keys:\n",
    "        df = dataframes[matching_keys[0]]\n",
    "        avg_demand = pd.to_numeric(df['value'], errors='coerce').mean()\n",
    "        print(f\"{state}: {avg_demand:.0f} MWh average demand\")\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# 10. FINAL STATUS AND NEXT STEPS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ IMPORT COMPLETE - NEXT STEPS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if dataframes:\n",
    "    print(f\"\"\"\n",
    "üéâ SUCCESS! Your electricity data has been imported.\n",
    "\n",
    "üìä WHAT YOU HAVE:\n",
    "   ‚Ä¢ {len(dataframes)} DataFrames ready for analysis\n",
    "   ‚Ä¢ Data for {len(state_stats) if 'state_stats' in locals() else 'multiple'} states\n",
    "   ‚Ä¢ Hourly data from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\n",
    "   ‚Ä¢ Four data types: Demand, Coal, Natural Gas, Wind\n",
    "\n",
    "üîß TO GET MORE DATA:\n",
    "   1. Increase date range (up to 5000 records limit):\n",
    "      start_date = end_date - timedelta(days=208)  # Max ~208 days\n",
    "   \n",
    "   2. Try additional BA codes:\n",
    "      # Add these to state_ba_mapping\n",
    "      \"New York\": [\"NY\", \"NYIS\", \"PJM\"],\n",
    "      \"Pennsylvania\": [\"PJM\", \"FE\", \"NY\"],\n",
    "      \"Virginia\": [\"PJM\", \"NY\"]\n",
    "\n",
    "üìà READY FOR ANALYSIS:\n",
    "   Start with: dataframes['electricity_demand_California_CAL'].head()\n",
    "\"\"\")\n",
    "else:\n",
    "    print(\"\"\"\n",
    "‚ö†Ô∏è  LIMITED OR NO DATA IMPORTED\n",
    "   \n",
    "   Suggestions:\n",
    "   1. Check which BA codes actually returned data\n",
    "   2. Try with known working codes: CAL, ERCO, SOCO, FE, FPL\n",
    "   3. Reduce date range to 5-7 days for testing\n",
    "   4. Check API response for error messages\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üß™ QUICK TEST - RUN THESE COMMANDS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "# Test 1: Check what data we have\n",
    "print(f\"Total DataFrames: {len(dataframes)}\")\n",
    "\n",
    "# Test 2: List all Texas data\n",
    "texas_data = [k for k in dataframes.keys() if 'Texas' in k]\n",
    "print(f\"Texas datasets: {len(texas_data)}\")\n",
    "for key in texas_data[:5]:\n",
    "    print(f\"  ‚Ä¢ {key}\")\n",
    "\n",
    "# Test 3: View a sample DataFrame\n",
    "if dataframes:\n",
    "    sample_key = list(dataframes.keys())[0]\n",
    "    print(f\"\\nSample: {sample_key}\")\n",
    "    print(dataframes[sample_key].head(2))\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ ALL DONE! YOUR DATA IS READY FOR ANALYSIS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9b2668d7-5424-4a61-b635-7afcf4e12ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d74b9043-4b5c-4378-846b-f28286464144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Resumo da Importa√ß√£o de Dados de Energia El√©trica - API EIA\n",
    "üìä Vis√£o Geral da Importa√ß√£o\n",
    "Per√≠odo Analisado: 21 de novembro a 21 de dezembro de 2025 (30 dias)\n",
    "\n",
    "Estados Coletados: 8 dos 10 estados-alvo\n",
    "\n",
    "‚úÖ Sucesso: Calif√≥rnia, Texas, Fl√≥rida, Ge√≥rgia, Illinois, Nova York, Carolina do Norte, Ohio\n",
    "\n",
    "‚ö†Ô∏è Faltantes: Pensilv√¢nia, Virg√≠nia\n",
    "\n",
    "üìà Dados Coletados\n",
    "Total Geral:\n",
    "30.931 registros de dados hor√°rios\n",
    "\n",
    "129 DataFrames criados\n",
    "\n",
    "86 datasets importados da API\n",
    "\n",
    "43 de 48 tentativas bem-sucedidas (90% de sucesso)\n",
    "\n",
    "Dados por Tipo de Energia:\n",
    "Demanda El√©trica: 8.652 registros (12 datasets)\n",
    "\n",
    "Gera√ß√£o a G√°s Natural: 8.628 registros (12 datasets)\n",
    "\n",
    "Gera√ß√£o a Carv√£o: 7.907 registros (11 datasets)\n",
    "\n",
    "Gera√ß√£o E√≥lica: 5.744 registros (8 datasets)\n",
    "\n",
    "Distribui√ß√£o por Estado:\n",
    "Calif√≥rnia e Nova York: 5.768 registros cada (dados mais completos)\n",
    "\n",
    "Carolina do Norte: 4.326 registros\n",
    "\n",
    "Fl√≥rida: 3.605 registros\n",
    "\n",
    "Ge√≥rgia, Illinois, Ohio: ~2.884 registros cada\n",
    "\n",
    "Texas: 2.812 registros (registros parcialmente completos)\n",
    "\n",
    "‚ö° Cobertura de Dados por Estado\n",
    "‚úÖ Estados com Dados Completos (4 tipos):\n",
    "Calif√≥rnia (CAL, CISO) - Modelo de \"Duck Curve\"\n",
    "\n",
    "Texas (ERCO) - Maior participa√ß√£o e√≥lica\n",
    "\n",
    "Nova York (NY, NYIS) - Zero carv√£o na matriz\n",
    "\n",
    "Illinois (MISO) - Maior demanda m√©dia\n",
    "\n",
    "Ge√≥rgia (SOCO) - Significativa gera√ß√£o a carv√£o\n",
    "\n",
    "Ohio (PJM) - Maior gera√ß√£o a carv√£o\n",
    "\n",
    "‚ö†Ô∏è Estados com Dados Parciais:\n",
    "Carolina do Norte: Sem dados de gera√ß√£o e√≥lica\n",
    "\n",
    "Fl√≥rida: FPL sem dados de carv√£o e e√≥lica\n",
    "\n",
    "üîç Insights Iniciais dos Dados\n",
    "Padr√µes de Demanda Identificados:\n",
    "Calif√≥rnia: Vale de demanda √†s 14-15h (efeito solar extremo)\n",
    "\n",
    "Texas: Maior gera√ß√£o e√≥lica (at√© 16.328 MWh)\n",
    "\n",
    "Illinois: Maior demanda m√©dia (77.029 MWh)\n",
    "\n",
    "Nova York/Calif√≥rnia: Gera√ß√£o zero ou m√≠nima de carv√£o\n",
    "\n",
    "Faixas de Valores por Estado:\n",
    "Demanda M√°xima: Illinois (99.092 MWh)\n",
    "\n",
    "Demanda M√≠nima: Fl√≥rida-FPC (3.756 MWh)\n",
    "\n",
    "Gera√ß√£o Carv√£o M√°xima: Illinois (24.222 MWh)\n",
    "\n",
    "Gera√ß√£o E√≥lica M√°xima: Texas (16.328 MWh)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d40c2543-c758-4135-ba05-743d8f9e0a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 3: BRONZE LAYER SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef33c51e-a841-4e71-80f6-e8d021991050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A Bronze Layer foi criada como o reposit√≥rio inicial dos dados brutos da API da EIA, preservando a forma original de 30.931 registros hor√°rios de demanda e gera√ß√£o el√©trica (carv√£o, g√°s natural e e√≥lica) para 8 estados norte-americanos no per√≠odo de novembro a dezembro de 2025. Esta camada mant√©m a integridade dos dados originais sem transforma√ß√µes, servindo como fonte √∫nica de verdade para auditoria e reprocessamento futuro, enquanto fornece a base para as transforma√ß√µes subsequentes nas camadas Silver e Gold do pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b281feed-b6b8-4857-a2ce-e8eeeaa20c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BRONZE LAYER INGESTION FOR COMMUNITY EDITION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ü•â BRONZE LAYER - DATABRICKS COMMUNITY EDITION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. PREPARE HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "print(\"\\nüîß PREPARING DATA FOR BRONZE LAYER\")\n",
    "\n",
    "def extract_metadata_from_key(key):\n",
    "    \"\"\"Extract state, data_type, and BA code from DataFrame key\"\"\"\n",
    "    key_lower = key.lower()\n",
    "    \n",
    "    # Extract data type\n",
    "    if 'demand' in key_lower or 'electricity' in key_lower:\n",
    "        data_type = 'demand'\n",
    "    elif 'coal' in key_lower:\n",
    "        data_type = 'coal'\n",
    "    elif 'gas' in key_lower or 'natural' in key_lower:\n",
    "        data_type = 'gas'\n",
    "    elif 'wind' in key_lower:\n",
    "        data_type = 'wind'\n",
    "    else:\n",
    "        data_type = 'unknown'\n",
    "    \n",
    "    # Extract state\n",
    "    state = 'Unknown'\n",
    "    states = {\n",
    "        'California': ['california', 'cal', 'ciso'],\n",
    "        'Texas': ['texas', 'erco'],\n",
    "        'Florida': ['florida', 'fl', 'fpl', 'fpc'],\n",
    "        'Ohio': ['ohio', 'oh', 'pjm'],\n",
    "        'Georgia': ['georgia', 'ga', 'soco'],\n",
    "        'New_York': ['new_york', 'ny', 'nyis'],\n",
    "        'North_Carolina': ['north_carolina', 'nc', 'car', 'duk'],\n",
    "        'Illinois': ['illinois', 'il', 'miso'],\n",
    "        'Pennsylvania': ['pennsylvania', 'pa'],\n",
    "        'Virginia': ['virginia', 'va']\n",
    "    }\n",
    "    \n",
    "    for state_name, keywords in states.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in key_lower:\n",
    "                state = state_name.replace('_', ' ')\n",
    "                break\n",
    "    \n",
    "    # Extract BA code\n",
    "    ba_code = 'Unknown'\n",
    "    ba_codes = ['ERCO', 'PJM', 'MISO', 'CISO', 'CAL', 'NY', 'NYIS', 'SOCO', 'DUK', 'CAR', 'FPL', 'FPC', 'TECO']\n",
    "    for code in ba_codes:\n",
    "        if code in key.upper():\n",
    "            ba_code = code\n",
    "            break\n",
    "    \n",
    "    return state, data_type, ba_code\n",
    "\n",
    "# ============================================================================\n",
    "# 2. PROCESS EACH DATAFRAME\n",
    "# ============================================================================\n",
    "print(\"\\nüì• PROCESSING DATAFRAMES INTO BRONZE LAYER\")\n",
    "\n",
    "bronze_views = {}\n",
    "processed_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "print(f\"\\nTotal DataFrames to process: {len(dataframes)}\")\n",
    "\n",
    "# Group DataFrames by type for organized processing\n",
    "dataframes_by_type = {'demand': [], 'coal': [], 'gas': [], 'wind': []}\n",
    "\n",
    "for df_key in dataframes.keys():\n",
    "    state, data_type, ba_code = extract_metadata_from_key(df_key)\n",
    "    if data_type in dataframes_by_type:\n",
    "        dataframes_by_type[data_type].append(df_key)\n",
    "\n",
    "print(\"\\nüìä DataFrames by type:\")\n",
    "for data_type, df_list in dataframes_by_type.items():\n",
    "    print(f\"  ‚Ä¢ {data_type.upper():10}: {len(df_list)} DataFrames\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CREATE BRONZE VIEWS BY DATA TYPE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîÑ CREATING BRONZE VIEWS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Process each data type separately\n",
    "for data_type, df_keys in dataframes_by_type.items():\n",
    "    if not df_keys:\n",
    "        print(f\"\\nüì≠ No DataFrames found for {data_type}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüìÇ Processing {data_type.upper()} DataFrames ({len(df_keys)}):\")\n",
    "    \n",
    "    # List to hold processed DataFrames of this type\n",
    "    processed_dfs = []\n",
    "    \n",
    "    for df_key in df_keys:\n",
    "        try:\n",
    "                      # Get the DataFrame (might be pandas or Spark)\n",
    "            df = dataframes[df_key]\n",
    "            \n",
    "            # Convert pandas DataFrame to Spark DataFrame if needed\n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                spark_df = spark.createDataFrame(df)\n",
    "            else:\n",
    "                spark_df = df  # Assume it's already a Spark DataFrame\n",
    "            \n",
    "            # Extract metadata\n",
    "            state, data_type_from_key, ba_code = extract_metadata_from_key(df_key)\n",
    "            \n",
    "            # Add metadata columns\n",
    "            bronze_df = spark_df.withColumn(\"data_type\", lit(data_type_from_key)) \\\n",
    "                                .withColumn(\"state\", lit(state)) \\\n",
    "                                .withColumn(\"ba_code\", lit(ba_code)) \\\n",
    "                                .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "                                .withColumn(\"source_file\", lit(df_key)) \\\n",
    "                                .withColumn(\"record_id\", lit(f\"{df_key}_{current_timestamp()}\"))\n",
    "            \n",
    "            # Standardize column names if needed\n",
    "            column_mapping = {\n",
    "                'respondent-name': 'respondent_name',\n",
    "                'type-name': 'type_name', \n",
    "                'value-units': 'value_units'\n",
    "            }\n",
    "            \n",
    "            for old_col, new_col in column_mapping.items():\n",
    "                if old_col in bronze_df.columns:\n",
    "                    bronze_df = bronze_df.withColumnRenamed(old_col, new_col)\n",
    "            \n",
    "            processed_dfs.append(bronze_df)\n",
    "            print(f\"  ‚úÖ {df_key:50} - {spark_df.count():6} records\")\n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {df_key:50} - ERROR: {str(e)[:80]}\")\n",
    "            failed_count += 1\n",
    "    \n",
    "    # Create unified view for this data type\n",
    "    if processed_dfs:\n",
    "        try:\n",
    "            # Union all DataFrames of this type\n",
    "            from functools import reduce\n",
    "            from pyspark.sql import DataFrame\n",
    "            \n",
    "            # Start with first DataFrame\n",
    "            unified_df = processed_dfs[0]\n",
    "            \n",
    "            # Union with remaining DataFrames\n",
    "            for df in processed_dfs[1:]:\n",
    "                unified_df = unified_df.unionByName(df, allowMissingColumns=True)\n",
    "            \n",
    "            # Create temporary view\n",
    "            view_name = f\"bronze_{data_type}\"\n",
    "            unified_df.createOrReplaceTempView(view_name)\n",
    "            bronze_views[view_name] = unified_df.count()\n",
    "            \n",
    "            print(f\"\\n  üéØ Created view: {view_name}\")\n",
    "            print(f\"    Total records: {unified_df.count():,}\")\n",
    "            print(f\"    Columns: {', '.join(unified_df.columns[:8])}...\")\n",
    "            \n",
    "            # Show sample\n",
    "            print(f\"    Sample (first 2 rows):\")\n",
    "            unified_df.limit(2).show(truncate=50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed to create {data_type} view: {str(e)[:100]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. CREATE UNIFIED BRONZE VIEW\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üåâ CREATING UNIFIED BRONZE VIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all bronze views\n",
    "bronze_view_names = [f\"bronze_{dt}\" for dt in ['demand', 'coal', 'gas', 'wind']]\n",
    "\n",
    "# Check which views exist\n",
    "existing_views = []\n",
    "for view_name in bronze_view_names:\n",
    "    try:\n",
    "        spark.sql(f\"SELECT 1 FROM {view_name} LIMIT 1\")\n",
    "        existing_views.append(view_name)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if existing_views:\n",
    "    print(f\"\\nüìã Available bronze views: {existing_views}\")\n",
    "    \n",
    "    # Build UNION query\n",
    "    union_parts = []\n",
    "    for view_name in existing_views:\n",
    "        union_parts.append(f\"SELECT * FROM {view_name}\")\n",
    "    \n",
    "    if union_parts:\n",
    "        union_query = \" UNION ALL \".join(union_parts)\n",
    "        \n",
    "        # Create unified view\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW bronze_all_data AS\n",
    "            {union_query}\n",
    "        \"\"\")\n",
    "        \n",
    "        # Get count\n",
    "        total_records = spark.sql(\"SELECT COUNT(*) as total FROM bronze_all_data\").collect()[0]['total']\n",
    "        \n",
    "        print(f\"\\n‚úÖ Created unified view: bronze_all_data\")\n",
    "        print(f\"   Total records: {total_records:,}\")\n",
    "        print(f\"   Combined from: {len(existing_views)} data types\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No bronze views available to unify\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. CREATE METADATA VIEW\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä CREATING METADATA VIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create metadata DataFrame\n",
    "metadata_data = []\n",
    "for view_name, count in bronze_views.items():\n",
    "    metadata_data.append({\n",
    "        'view_name': view_name,\n",
    "        'record_count': count,\n",
    "        'data_type': view_name.replace('bronze_', ''),\n",
    "        'created_at': pd.Timestamp.now()\n",
    "    })\n",
    "\n",
    "if metadata_data:\n",
    "    metadata_df = spark.createDataFrame(metadata_data)\n",
    "    metadata_df.createOrReplaceTempView(\"bronze_metadata\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created metadata view: bronze_metadata\")\n",
    "    print(\"\\nüìã Metadata content:\")\n",
    "    metadata_df.show(truncate=False)\n",
    "else:\n",
    "    print(\"\\n‚ùå No metadata to create\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. VERIFY AND SUMMARIZE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîç VERIFYING BRONZE LAYER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# List all available bronze views\n",
    "print(\"\\nüìã ALL BRONZE VIEWS:\")\n",
    "all_views = [v for v in spark.catalog.listTables() if v.name.startswith('bronze_')]\n",
    "for view in all_views:\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {view.name}\").collect()[0]['cnt']\n",
    "        print(f\"  ‚Ä¢ {view.name:25} - {count:8,} records\")\n",
    "    except:\n",
    "        print(f\"  ‚Ä¢ {view.name:25} - ERROR\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. RUN DATA QUALITY CHECKS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üß™ DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'bronze_all_data' in [v.name for v in spark.catalog.listTables()]:\n",
    "    print(\"\\nüìà DATA QUALITY METRICS:\")\n",
    "    \n",
    "    # 1. Check for missing values\n",
    "    print(\"\\n1. Missing Values Check:\")\n",
    "    result = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_records,\n",
    "            SUM(CASE WHEN period IS NULL THEN 1 ELSE 0 END) as missing_period,\n",
    "            SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END) as missing_value,\n",
    "            SUM(CASE WHEN state = 'Unknown' THEN 1 ELSE 0 END) as unknown_state\n",
    "        FROM bronze_all_data\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Total records: {result['total_records']:,}\")\n",
    "    print(f\"   ‚Ä¢ Missing period: {result['missing_period']:,} ({result['missing_period']/result['total_records']*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Missing value: {result['missing_value']:,} ({result['missing_value']/result['total_records']*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Unknown state: {result['unknown_state']:,} ({result['unknown_state']/result['total_records']*100:.1f}%)\")\n",
    "    \n",
    "    # 2. Check data distribution\n",
    "    print(\"\\n2. Data Distribution by Type:\")\n",
    "    dist_result = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            data_type,\n",
    "            COUNT(*) as record_count,\n",
    "            COUNT(DISTINCT state) as state_count,\n",
    "            COUNT(DISTINCT ba_code) as ba_count,\n",
    "            MIN(period) as earliest,\n",
    "            MAX(period) as latest\n",
    "        FROM bronze_all_data\n",
    "        GROUP BY data_type\n",
    "        ORDER BY record_count DESC\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    for row in dist_result:\n",
    "        print(f\"   ‚Ä¢ {row['data_type']:10}: {row['record_count']:6,} records, {row['state_count']} states, {row['ba_count']} BA codes\")\n",
    "    \n",
    "    # 3. Check value ranges\n",
    "    print(\"\\n3. Value Ranges by Data Type:\")\n",
    "    value_ranges = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            data_type,\n",
    "            MIN(CAST(value AS DOUBLE)) as min_value,\n",
    "            MAX(CAST(value AS DOUBLE)) as max_value,\n",
    "            AVG(CAST(value AS DOUBLE)) as avg_value\n",
    "        FROM bronze_all_data\n",
    "        WHERE value IS NOT NULL AND value != ''\n",
    "        GROUP BY data_type\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    for row in value_ranges:\n",
    "        print(f\"   ‚Ä¢ {row['data_type']:10}: Min={row['min_value']:.0f}, Max={row['max_value']:.0f}, Avg={row['avg_value']:.0f}\")\n",
    "    \n",
    "    # 4. Show sample queries\n",
    "    print(\"\\n4. Sample Data:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            data_type,\n",
    "            state,\n",
    "            ba_code,\n",
    "            period,\n",
    "            value\n",
    "        FROM bronze_all_data\n",
    "        WHERE period LIKE '2025-12-20%'\n",
    "        ORDER BY period\n",
    "        LIMIT 5\n",
    "    \"\"\").show(truncate=False)\n",
    "\n",
    "# ============================================================================\n",
    "# # ============================================================================\n",
    "# 8. FINAL SUMMARY AND USAGE GUIDE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ BRONZE LAYER COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f'''\n",
    "üéâ BRONZE LAYER SUCCESSFULLY CREATED!\n",
    "\n",
    "üìä INGESTION SUMMARY:\n",
    "   ‚Ä¢ Processed: {processed_count} DataFrames successfully\n",
    "   ‚Ä¢ Failed: {failed_count} DataFrames\n",
    "   ‚Ä¢ Created {len(bronze_views)} bronze views\n",
    "   ‚Ä¢ Total records in unified view: {total_records if 'total_records' in locals() else 0:,}\n",
    "\n",
    "üìã AVAILABLE VIEWS:\n",
    "   ‚Ä¢ bronze_all_data      - All unified data\n",
    "   ‚Ä¢ bronze_metadata      - Metadata about bronze layer\n",
    "   ‚Ä¢ bronze_demand        - Electricity demand data\n",
    "   ‚Ä¢ bronze_coal          - Coal generation data  \n",
    "   ‚Ä¢ bronze_gas           - Natural gas generation data\n",
    "   ‚Ä¢ bronze_wind          - Wind generation data\n",
    "\n",
    "üîç HOW TO USE YOUR BRONZE DATA:\n",
    "\n",
    "1. Query specific data type:\n",
    "   display(spark.sql('SELECT * FROM bronze_demand LIMIT 5'))\n",
    "\n",
    "2. Analyze by state:\n",
    "   display(spark.sql(\n",
    "        \"SELECT state, COUNT(*) as records \"\n",
    "        \"FROM bronze_all_data \"\n",
    "        \"GROUP BY state \"\n",
    "        \"ORDER BY records DESC\"\n",
    "   ))\n",
    "\n",
    "3. Check data quality:\n",
    "   display(spark.sql(\n",
    "        \"SELECT \"\n",
    "        \"    data_type, \"\n",
    "        \"    COUNT(*) as total, \"\n",
    "        \"    SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END) as null_values \"\n",
    "        \"FROM bronze_all_data \"\n",
    "        \"GROUP BY data_type\"\n",
    "   ))\n",
    "\n",
    "4. Time series analysis:\n",
    "   display(spark.sql(\n",
    "        \"SELECT \"\n",
    "        \"    DATE(period) as date, \"\n",
    "        \"    AVG(CAST(value AS DOUBLE)) as avg_value \"\n",
    "        \"FROM bronze_all_data \"\n",
    "        \"WHERE data_type = 'demand' \"\n",
    "        \"GROUP BY DATE(period) \"\n",
    "        \"ORDER BY date\"\n",
    "   ))\n",
    "\n",
    "üß™ QUICK TEST QUERIES:\n",
    "''')\n",
    "\n",
    "# Run a quick test\n",
    "try:\n",
    "    print(\"Test 1: Count by data type\")\n",
    "    display(\n",
    "        spark.sql(\n",
    "            \"\"\"\n",
    "SELECT data_type, COUNT(*) as count\n",
    "FROM bronze_all_data\n",
    "GROUP BY data_type\n",
    "ORDER BY count DESC\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\\nTest 2: States with most data\")\n",
    "    display(\n",
    "        spark.sql(\n",
    "            \"\"\"\n",
    "SELECT state, COUNT(*) as records\n",
    "FROM bronze_all_data\n",
    "WHERE state != 'Unknown'\n",
    "GROUP BY state\n",
    "ORDER BY records DESC\n",
    "LIMIT 5\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\\nTest 3: Latest data sample\")\n",
    "    display(\n",
    "        spark.sql(\n",
    "            \"\"\"\n",
    "SELECT \n",
    "    data_type,\n",
    "    state,\n",
    "    period,\n",
    "    value\n",
    "FROM bronze_all_data\n",
    "WHERE period LIKE '2025-12-20%'\n",
    "ORDER BY period DESC\n",
    "LIMIT 3\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Test queries failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üöÄ READY FOR SILVER LAYER PROCESSING!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nYour bronze layer is now available as temporary views.\")\n",
    "print(\"Proceed to create the Silver Layer for data cleaning and transformation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "384b2419-258c-41cb-a445-b29be28b0190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 4: SILVER LAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37a96396-5857-4d96-a3bc-3ad395626d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A Silver Layer realizou a limpeza, padroniza√ß√£o e enriquecimento dos dados brutos, convertendo valores para formato num√©rico, tratando dados ausentes, padronizando nomes de estados e unidades de medida, al√©m de adicionar colunas temporais (timestamp, data, hora) para an√°lise temporal. Esta camada de qualidade garantida unificou 129 DataFrames em uma estrutura consolidada pronta para modelagem anal√≠tica, mantendo rastreabilidade atrav√©s de metadados de processamento enquanto prepara os dados para consumo na camada Gold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5674f50c-2304-4f80-b0f8-1210a029a6b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ü•à SILVER LAYER - FIXED FOR ISO FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ü•à SILVER LAYER - FIXED FOR ISO FORMAT (YYYY-MM-DDTHH)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clean up\n",
    "print(\"\\nüßπ Cleaning up existing silver views...\")\n",
    "existing_silver_views = [v.name for v in spark.catalog.listTables() \n",
    "                         if v.name.startswith('silver_')]\n",
    "for view in existing_silver_views:\n",
    "    try:\n",
    "        spark.sql(f\"DROP VIEW IF EXISTS {view}\")\n",
    "        print(f\"  ‚úÖ Dropped: {view}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Create silver_all_data with the CORRECT format\n",
    "print(\"\\nüîÑ Creating silver_all_data with ISO format parsing...\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW silver_all_data AS\n",
    "    SELECT\n",
    "        -- Parse the ISO format: 2025-11-21T00 ‚Üí 2025-11-21 00:00:00\n",
    "        CASE \n",
    "            WHEN period LIKE '____-__-__T__' AND LENGTH(period) = 13\n",
    "            THEN to_timestamp(\n",
    "                CONCAT(\n",
    "                    SUBSTRING(period, 1, 10),  -- Date part: 2025-11-21\n",
    "                    ' ',\n",
    "                    SUBSTRING(period, 12, 2),  -- Hour part: 00\n",
    "                    ':00:00'                    -- Add minutes and seconds\n",
    "                ),\n",
    "                'yyyy-MM-dd HH:mm:ss'\n",
    "            )\n",
    "            ELSE NULL\n",
    "        END AS period_ts,\n",
    "        \n",
    "        -- Extract date part\n",
    "        CASE \n",
    "            WHEN period LIKE '____-__-__T__' AND LENGTH(period) = 13\n",
    "            THEN to_date(SUBSTRING(period, 1, 10), 'yyyy-MM-dd')\n",
    "            ELSE NULL\n",
    "        END AS period_date,\n",
    "        \n",
    "        -- Original period for reference\n",
    "        period,\n",
    "        \n",
    "        -- Extract hour from period (e.g., get '00' from '2025-11-21T00')\n",
    "        CASE \n",
    "            WHEN period LIKE '____-__-__T__' AND LENGTH(period) = 13\n",
    "            THEN CAST(SUBSTRING(period, 12, 2) AS INT)\n",
    "            ELSE NULL\n",
    "        END AS period_hour,\n",
    "        \n",
    "        -- Clean the value\n",
    "        CASE \n",
    "            WHEN TRIM(value) = '' THEN NULL\n",
    "            WHEN TRY_CAST(value AS DOUBLE) IS NOT NULL \n",
    "            AND TRY_CAST(value AS DOUBLE) >= 0 \n",
    "            THEN CAST(value AS DOUBLE)\n",
    "            ELSE NULL \n",
    "        END AS value_cleaned,\n",
    "        \n",
    "        -- Original value\n",
    "        value AS value_original,\n",
    "        \n",
    "        -- All other columns\n",
    "        data_type,\n",
    "        state,\n",
    "        ba_code,\n",
    "        COALESCE(respondent_name, 'Unknown') AS respondent_name,\n",
    "        COALESCE(type_name, 'Unknown') AS type_name,\n",
    "        COALESCE(value_units, 'Unknown') AS value_units,\n",
    "        source_file,\n",
    "        ingestion_timestamp,\n",
    "        record_id,\n",
    "        \n",
    "        -- Processing metadata\n",
    "        CURRENT_TIMESTAMP() AS silver_processing_timestamp\n",
    "        \n",
    "    FROM bronze_all_data\n",
    "    WHERE period IS NOT NULL\n",
    "    AND value IS NOT NULL\n",
    "    AND period LIKE '____-__-__T__'  -- Ensure it matches our expected format\n",
    "\"\"\")\n",
    "\n",
    "# Check what we created\n",
    "print(\"\\n‚úÖ Created silver_all_data\")\n",
    "print(\"\\nüìä Statistics:\")\n",
    "total_count = spark.sql(\"SELECT COUNT(*) FROM silver_all_data\").collect()[0][0]\n",
    "period_ts_count = spark.sql(\"SELECT COUNT(*) FROM silver_all_data WHERE period_ts IS NOT NULL\").collect()[0][0]\n",
    "\n",
    "print(f\"‚Ä¢ Total records: {total_count:,}\")\n",
    "print(f\"‚Ä¢ Records with timestamps: {period_ts_count:,} ({period_ts_count/total_count*100:.1f}%)\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüëÅÔ∏è  Sample data (with parsed timestamps):\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        period,\n",
    "        period_ts,\n",
    "        period_date,\n",
    "        period_hour,\n",
    "        state,\n",
    "        data_type,\n",
    "        value_cleaned,\n",
    "        value_units\n",
    "    FROM silver_all_data \n",
    "    WHERE period_ts IS NOT NULL\n",
    "    ORDER BY period_ts\n",
    "    LIMIT 10\n",
    "\"\"\"))\n",
    "\n",
    "# Create individual views by data type\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìÅ CREATING INDIVIDUAL SILVER VIEWS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for data_type in ['demand', 'coal', 'gas', 'wind']:\n",
    "    silver_view = f\"silver_{data_type}\"\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW {silver_view} AS\n",
    "        SELECT *\n",
    "        FROM silver_all_data\n",
    "        WHERE data_type = '{data_type}'\n",
    "    \"\"\")\n",
    "    count = spark.sql(f\"SELECT COUNT(*) FROM {silver_view}\").collect()[0][0]\n",
    "    print(f\"‚úÖ {silver_view:20} - {count:8,} records\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ SILVER LAYER COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úÖ Timestamps successfully parsed from ISO format: YYYY-MM-DDTHH\")\n",
    "print(f\"‚úÖ Added period_hour column for easy hour-based analysis\")\n",
    "print(f\"‚úÖ Ready for Gold layer star schema creation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a21f0674-7d8a-4faf-872d-21f6ee5b60a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "STEP 5: GOLD LAYER - STAR SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "340a9e7e-9f5e-4055-a73a-8f0da6b223f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üèÜ Gold Layer - Camada de Modelagem Anal√≠tica\n",
    "A Gold Layer representa a camada final de modelagem dimensional, implementando um esquema estrela otimizado para an√°lise de desempenho e consumo pela camada de visualiza√ß√£o (Power BI). Esta camada transforma os dados da Silver em tabelas factuais e dimensionais permanentes, organizadas para consultas anal√≠ticas eficientes atrav√©s do Databricks SQL Serverless.\n",
    "\n",
    "‚≠ê Esquema Estrela Implementado\n",
    "üåü Tabela Fato Principal: gold_fact_energy_measurements\n",
    "Prop√≥sito: Registros detalhados de medi√ß√µes hor√°rias\n",
    "\n",
    "Cont√©m: 721 registros hor√°rios √ó 8 estados √ó 4 tipos de dados\n",
    "\n",
    "Campos-chave:\n",
    "\n",
    "measurement_id: Identificador √∫nico (monotonically_increasing_id)\n",
    "\n",
    "measurement_timestamp: Data/hora completa da medi√ß√£o\n",
    "\n",
    "measurement_date e measurement_hour: Componentes temporais\n",
    "\n",
    "state: Estado (California, Texas, Florida, etc.)\n",
    "\n",
    "ba_code: C√≥digo da autoridade de balanceamento (CISO, ERCO, PJM, etc.)\n",
    "\n",
    "data_type: Tipo de dado (demand, coal, gas, wind)\n",
    "\n",
    "measurement_value: Valor limpo em MWh\n",
    "\n",
    "original_value: Valor bruto preservado\n",
    "\n",
    "value_units: Unidades de medida (megawatthours)\n",
    "\n",
    "Metadados: source_file, ingestion_timestamp, silver_processing_timestamp\n",
    "\n",
    "üìä Tabela Fato Agregada: gold_fact_daily_energy\n",
    "Prop√≥sito: Agrega√ß√µes di√°rias otimizadas para visualiza√ß√µes\n",
    "\n",
    "Cont√©m: Agrega√ß√µes di√°rias por estado e tipo de energia\n",
    "\n",
    "M√©tricas:\n",
    "\n",
    "total_value: Soma di√°ria em MWh\n",
    "\n",
    "avg_value: M√©dia hor√°ria di√°ria\n",
    "\n",
    "min_value/max_value: Valores extremos do dia\n",
    "\n",
    "measurement_count: N√∫mero de medi√ß√µes hor√°rias\n",
    "\n",
    "Dimens√µes: date, state, data_type\n",
    "\n",
    "üóìÔ∏è Dimens√£o de Data: gold_dim_date\n",
    "Cont√©m: Todos os dias presentes nos dados factuais\n",
    "\n",
    "Atributos:\n",
    "\n",
    "date: Data no formato YYYY-MM-DD\n",
    "\n",
    "year, month, day: Componentes num√©ricos\n",
    "\n",
    "quarter: Trimestre (1-4)\n",
    "\n",
    "month_name, day_name: Nomes por extenso\n",
    "\n",
    "day_of_week, day_of_year: Refer√™ncias temporais\n",
    "\n",
    "weekday_weekend: Classifica√ß√£o dia √∫til/fim de semana\n",
    "\n",
    "üìç Dimens√£o de Localiza√ß√£o: gold_dim_location\n",
    "Cont√©m: Estados √∫nicos e seus c√≥digos BA\n",
    "\n",
    "Atributos:\n",
    "\n",
    "location_id: Identificador num√©rico sequencial\n",
    "\n",
    "state: Nome do estado (10 estados inclu√≠dos)\n",
    "\n",
    "ba_code: C√≥digo da autoridade de balanceamento\n",
    "\n",
    "market_category: Classifica√ß√£o (Major Market/Other Market)\n",
    "\n",
    "Major Market: California, Texas, Florida, New York\n",
    "\n",
    "Other Market: Demais estados\n",
    "\n",
    "‚ö° Dimens√£o de Tipo de Energia: gold_dim_energy_type\n",
    "Cont√©m: Tipos de dados energ√©ticos √∫nicos\n",
    "\n",
    "Atributos:\n",
    "\n",
    "energy_type_id: Identificador num√©rico sequencial\n",
    "\n",
    "data_type: C√≥digo (demand, coal, gas, wind)\n",
    "\n",
    "energy_type_name: Descri√ß√£o por extenso\n",
    "\n",
    "demand ‚Üí \"Electricity Demand\"\n",
    "\n",
    "coal ‚Üí \"Coal Generation\"\n",
    "\n",
    "gas ‚Üí \"Natural Gas Generation\"\n",
    "\n",
    "wind ‚Üí \"Wind Generation\"\n",
    "\n",
    "fuel_category: Classifica√ß√£o por categoria de combust√≠vel\n",
    "\n",
    "Fossil Fuel: coal, gas\n",
    "\n",
    "Renewable: wind\n",
    "\n",
    "Load: demand\n",
    "\n",
    "Other: demais tipos\n",
    "\n",
    "üéØ Objetivos do Esquema Estrela\n",
    "Desempenho: Consultas r√°pidas atrav√©s de jun√ß√µes simples\n",
    "\n",
    "Simplicidade: Modelo intuitivo para usu√°rios de neg√≥cio\n",
    "\n",
    "Flexibilidade: Suporte a m√∫ltiplos n√≠veis de granularidade\n",
    "\n",
    "Consist√™ncia: Dimens√µes compartilhadas entre fatos\n",
    "\n",
    "Hist√≥rico: Preserva√ß√£o de dados hist√≥ricos para tend√™ncias\n",
    "\n",
    "üìà Casos de Uso Habilitados\n",
    "An√°lise Temporal:\n",
    "Tend√™ncias hor√°rias/di√°rias de demanda\n",
    "\n",
    "Sazonalidade por dia da semana\n",
    "\n",
    "Padr√µes \"Duck Curve\" (Calif√≥rnia)\n",
    "\n",
    "An√°lise Geogr√°fica:\n",
    "Comparativo entre estados\n",
    "\n",
    "Distribui√ß√£o por regi√£o\n",
    "\n",
    "Performance por autoridade de balanceamento\n",
    "\n",
    "An√°lise de Mix Energ√©tico:\n",
    "Participa√ß√£o de fontes (carv√£o, g√°s, e√≥lica)\n",
    "\n",
    "Transi√ß√£o energ√©tica por estado\n",
    "\n",
    "Correla√ß√£o demanda-gera√ß√£o\n",
    "\n",
    "Business Intelligence:\n",
    "KPIs de desempenho do grid\n",
    "\n",
    "Alertas de picos de demanda\n",
    "\n",
    "Planejamento de capacidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "08269c4b-7d92-407f-ab15-e0c454c49237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ü•á GOLD LAYER - PERMANENT TABLES FOR SERVERLESS SQL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ü•á GOLD LAYER - PERMANENT TABLES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Creating permanent tables for Serverless SQL access...\")\n",
    "\n",
    "# First, check what catalog/database we're in\n",
    "print(\"\\nüìã Current catalog/database:\")\n",
    "display(spark.sql(\"SELECT current_catalog(), current_database()\"))\n",
    "\n",
    "# ============================================================================\n",
    "# CLEANUP EXISTING TABLES\n",
    "# ============================================================================\n",
    "print(\"\\nüßπ Cleaning up existing tables...\")\n",
    "try:\n",
    "    spark.sql(\"DROP TABLE IF EXISTS gold_fact_energy_measurements\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS gold_fact_daily_energy\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS gold_dim_date\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS gold_dim_location\")\n",
    "    spark.sql(\"DROP TABLE IF EXISTS gold_dim_energy_type\")\n",
    "    print(\"‚úÖ Cleanup complete\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Cleanup warning: {str(e)[:50]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. MAIN FACT TABLE (PERMANENT TABLE)\n",
    "# ============================================================================\n",
    "print(\"\\n1. Creating permanent fact table...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE gold_fact_energy_measurements AS\n",
    "    SELECT\n",
    "        -- Generate a unique ID\n",
    "        monotonically_increasing_id() AS measurement_id,\n",
    "        \n",
    "        -- Time dimensions\n",
    "        period_ts AS measurement_timestamp,\n",
    "        period_date AS measurement_date,\n",
    "        period_hour AS measurement_hour,\n",
    "        \n",
    "        -- Location\n",
    "        state,\n",
    "        ba_code,\n",
    "        \n",
    "        -- Energy type\n",
    "        data_type,\n",
    "        \n",
    "        -- Values\n",
    "        value_cleaned AS measurement_value,\n",
    "        value_original AS original_value,\n",
    "        value_units,\n",
    "        \n",
    "        -- Additional info\n",
    "        respondent_name,\n",
    "        type_name,\n",
    "        source_file,\n",
    "        \n",
    "        -- Metadata\n",
    "        ingestion_timestamp,\n",
    "        silver_processing_timestamp\n",
    "        \n",
    "    FROM silver_all_data\n",
    "    WHERE value_cleaned IS NOT NULL\n",
    "      AND period_ts IS NOT NULL\n",
    "      AND period_date IS NOT NULL\n",
    "\"\"\")\n",
    "fact_count = spark.sql(\"SELECT COUNT(*) FROM gold_fact_energy_measurements\").collect()[0][0]\n",
    "print(f\"‚úÖ Created: gold_fact_energy_measurements ({fact_count:,} records)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DAILY AGGREGATED TABLE (FOR YOUR AREA CHART)\n",
    "# ============================================================================\n",
    "print(\"\\n2. Creating daily aggregated table for area chart...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE gold_fact_daily_energy AS\n",
    "    SELECT\n",
    "        measurement_date AS date,\n",
    "        state,\n",
    "        data_type,\n",
    "        SUM(measurement_value) AS total_value,\n",
    "        AVG(measurement_value) AS avg_value,\n",
    "        MIN(measurement_value) AS min_value,\n",
    "        MAX(measurement_value) AS max_value,\n",
    "        COUNT(*) AS measurement_count\n",
    "    FROM gold_fact_energy_measurements\n",
    "    WHERE measurement_value IS NOT NULL\n",
    "      AND measurement_date IS NOT NULL\n",
    "    GROUP BY measurement_date, state, data_type\n",
    "\"\"\")\n",
    "daily_count = spark.sql(\"SELECT COUNT(*) FROM gold_fact_daily_energy\").collect()[0][0]\n",
    "print(f\"‚úÖ Created: gold_fact_daily_energy ({daily_count:,} records)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DIMENSION TABLES (PERMANENT TABLES)\n",
    "# ============================================================================\n",
    "print(\"\\n3. Creating dimension tables...\")\n",
    "\n",
    "# Dim Date\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE gold_dim_date AS\n",
    "    WITH date_range AS (\n",
    "        SELECT DISTINCT measurement_date AS date\n",
    "        FROM gold_fact_energy_measurements\n",
    "        WHERE measurement_date IS NOT NULL\n",
    "    )\n",
    "    SELECT\n",
    "        date,\n",
    "        YEAR(date) AS year,\n",
    "        MONTH(date) AS month,\n",
    "        DAY(date) AS day,\n",
    "        DAYOFWEEK(date) AS day_of_week,\n",
    "        DAYOFYEAR(date) AS day_of_year,\n",
    "        QUARTER(date) AS quarter,\n",
    "        DATE_FORMAT(date, 'MMMM') AS month_name,\n",
    "        DATE_FORMAT(date, 'EEEE') AS day_name,\n",
    "        CASE \n",
    "            WHEN DAYOFWEEK(date) IN (1, 7) THEN 'Weekend'\n",
    "            ELSE 'Weekday'\n",
    "        END AS weekday_weekend\n",
    "    FROM date_range\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created: gold_dim_date\")\n",
    "\n",
    "# Dim Location\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE gold_dim_location AS\n",
    "    SELECT DISTINCT\n",
    "        ROW_NUMBER() OVER (ORDER BY state) AS location_id,\n",
    "        state,\n",
    "        ba_code,\n",
    "        CASE \n",
    "            WHEN state IN ('California', 'Texas', 'Florida', 'New York') \n",
    "            THEN 'Major Market'\n",
    "            ELSE 'Other Market'\n",
    "        END AS market_category\n",
    "    FROM gold_fact_energy_measurements\n",
    "    WHERE state IS NOT NULL AND state != 'Unknown'\n",
    "    ORDER BY state\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created: gold_dim_location\")\n",
    "\n",
    "# Dim Energy Type\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE gold_dim_energy_type AS\n",
    "    SELECT DISTINCT\n",
    "        ROW_NUMBER() OVER (ORDER BY data_type) AS energy_type_id,\n",
    "        data_type,\n",
    "        CASE \n",
    "            WHEN data_type = 'demand' THEN 'Electricity Demand'\n",
    "            WHEN data_type = 'coal' THEN 'Coal Generation'\n",
    "            WHEN data_type = 'gas' THEN 'Natural Gas Generation'\n",
    "            WHEN data_type = 'wind' THEN 'Wind Generation'\n",
    "            ELSE data_type\n",
    "        END AS energy_type_name,\n",
    "        CASE \n",
    "            WHEN data_type IN ('coal', 'gas') THEN 'Fossil Fuel'\n",
    "            WHEN data_type = 'wind' THEN 'Renewable'\n",
    "            WHEN data_type = 'demand' THEN 'Load'\n",
    "            ELSE 'Other'\n",
    "        END AS fuel_category\n",
    "    FROM gold_fact_energy_measurements\n",
    "    WHERE data_type IS NOT NULL\n",
    "    ORDER BY data_type\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created: gold_dim_energy_type\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PERMANENT GOLD LAYER TABLES CREATED!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "üéâ Permanent tables created! Access them from Serverless SQL with:\n",
    "\n",
    "1. Your area chart query:\n",
    "   SELECT \n",
    "     date,\n",
    "     state,\n",
    "     SUM(total_value) AS total_generation_mw\n",
    "   FROM gold_fact_daily_energy\n",
    "   WHERE data_type IN ('coal', 'gas', 'wind')\n",
    "     AND state IS NOT NULL\n",
    "     AND state != 'Unknown'\n",
    "   GROUP BY date, state\n",
    "   ORDER BY date, state;\n",
    "\n",
    "2. Test query:\n",
    "   SELECT * FROM gold_fact_daily_energy \n",
    "   WHERE data_type IN ('coal', 'gas', 'wind')\n",
    "   LIMIT 5;\n",
    "\n",
    "üìã Available permanent tables:\n",
    "   ‚Ä¢ gold_fact_energy_measurements\n",
    "   ‚Ä¢ gold_fact_daily_energy\n",
    "   ‚Ä¢ gold_dim_date\n",
    "   ‚Ä¢ gold_dim_location\n",
    "   ‚Ä¢ gold_dim_energy_type\n",
    "\"\"\")\n",
    "\n",
    "# Quick test\n",
    "print(\"\\nüß™ Quick test - sample from daily aggregated:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT * FROM gold_fact_daily_energy \n",
    "    WHERE data_type IN ('coal', 'gas', 'wind')\n",
    "    LIMIT 5\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MVP  STAR SCHEMA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
